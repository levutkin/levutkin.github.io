%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Lev Utkin at 2022-01-14 01:25:59 +0300 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}



@inproceedings{Tyree-etal-11,
	author = {S. Tyree and K.Q. Weinberger and K. Agrawal and J. Paykin},
	booktitle = {Proceedings of the 20th international conference on World wide web},
	date-added = {2022-01-14 01:22:41 +0300},
	date-modified = {2022-01-14 01:24:48 +0300},
	pages = {387--396},
	publisher = {ACM},
	title = {Parallel boosted regression trees for web search ranking},
	year = {2011}}

@book{Rokach-2019,
	author = {L. Rokach},
	date-added = {2019-05-19 10:44:19 +0300},
	date-modified = {2019-05-19 10:44:19 +0300},
	publisher = {World Scientific},
	title = {Ensemble Learning: Pattern Classification Using Ensemble Methods},
	volume = {85},
	year = {2019}}

@book{Alfaro-Gamez-Garcia-2019,
	author = {E. Alfaro and M. Gamez and N. Garcia},
	date-added = {2019-05-19 10:43:56 +0300},
	date-modified = {2019-05-19 10:43:56 +0300},
	publisher = {John Wiley \& Sons, Incorporated},
	title = {Ensemble Classification Methods with Applications in R},
	year = {2019}}

@article{Schapire-90,
	author = {R.E. Schapire},
	date-added = {2018-06-23 08:36:20 +0000},
	date-modified = {2018-06-23 08:37:54 +0000},
	journal = {Machine Learning},
	number = {2},
	pages = {197--227},
	title = {The strength of weak learnability},
	volume = {5},
	year = {1990.}}

@incollection{Bertoni-Campadelli-Parodi-97,
	address = {Berlin, Heidelberg},
	author = {A. Bertoni, P. Campadelli, M. Parodi},
	booktitle = {Artificial Neural Networks --- ICANN'97, Lecture Notes in Computer Science},
	journal = {Artificial Neural Networks},
	owner = {LVU},
	pages = {343-348},
	publisher = {Springer},
	timestamp = {2012.12.23},
	title = {A Boosting algorithm for regression},
	volume = {1327},
	year = {1997}}

@article{Avnimelech-Intrator-99,
	author = {R. Avnimelech and N. Intrator},
	journal = {Neural Computation},
	owner = {LVU},
	pages = {499-520},
	timestamp = {2013.01.08},
	title = {Boosting regression estimators},
	volume = {11},
	year = {1999}}

@incollection{Bannerman-Thompson-2013,
	affiliation = {Department of Environmental Health, University of Cincinnati, Cincinnati, OH 45267, United States; Department of Mathematics and Statistics, University of North Carolina, Wilmington, NC 28403, United States},
	author = {Bannerman-Thompson, H. and Bhaskara Rao, M. and Kasala, S.},
	author_keywords = {Bootstrapping; Classification trees; Logistic regression; Multiple regression; Out-of-bag error; Re-substitution error; Regression trees; Weighted bootstrapping},
	booktitle = {Handbook of Statistics},
	chapter = {5},
	document_type = {Article},
	doi = {10.1016/B978-0-444-53859-8.00005-9},
	editor = {C.R. Rao and Venu Govindaraju},
	journal = {Handbook of Statistics},
	owner = {LVU},
	pages = {101-149},
	publisher = {Elsevier},
	source = {Scopus},
	timestamp = {2014.11.13},
	title = {Bagging, boosting, and random forests using R},
	url = {http://www.sciencedirect.com/science/article/pii/B9780444538598000059},
	volume = {31},
	year = {2013},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/B9780444538598000059},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-444-53859-8.00005-9}}

@article{Bonato-2011,
	abstract = {Motivation: We propose a Bayesian ensemble method for survival prediction
	in high-dimensional gene expression data. We specify a fully Bayesian
	hierarchical approach based on an ensemble `sum-of-trees' model and
	illustrate our method using three popular survival models. Our non-parametric
	method incorporates both additive and interaction effects between
	genes, which results in high predictive accuracy compared with other
	methods. In addition, our method provides model-free variable selection
	of important prognostic markers based on controlling the false discovery
	rates; thus providing a unified procedure to select relevant genes
	and predict survivor functions.Results: We assess the performance
	of our method several simulated and real microarray datasets. We
	show that our method selects genes potentially related to the development
	of the disease as well as yields predictive performance that is very
	competitive to many other existing methods.Availability: http://works.bepress.com/veera/1/.Contact:
	veera@mdanderson.orgSupplementary Information: Supplementary data
	are available at Bioinformatics online.},
	author = {Bonato, Vinicius and Baladandayuthapani, Veerabhadran and Broom, Bradley M. and Sulman, Erik P. and Aldape, Kenneth D. and Do, Kim-Anh},
	journal = {Bioinformatics},
	number = {3},
	owner = {LVU},
	pages = {359-367},
	timestamp = {2013.01.11},
	title = {Bayesian ensemble methods for survival prediction in gene expression data},
	volume = {27},
	year = {2011}}

@article{Breiman-1996,
	author = {L. Breiman},
	journal = {Machine Learning},
	number = {2},
	owner = {lev u},
	pages = {123--140},
	timestamp = {2015.10.10},
	title = {Bagging predictors},
	volume = {24},
	year = {1996}}

@inproceedings{Bylander-Tate-06,
	author = {T. Bylander and L. Tate},
	booktitle = {Proceedings of the 19th International FLAIRS Conference},
	owner = {LVU},
	pages = {544-549},
	timestamp = {2012.09.30},
	title = {Using validation sets to avoid overfitting in {A}da{B}oost},
	year = {2006}}

@article{Shrestha-Solomatine-06,
	author = {D.L. Shrestha, D.P. Solomatine},
	journal = {Neural Computation},
	number = {7},
	owner = {LVU},
	pages = {1678-1710},
	timestamp = {2012.12.23},
	title = {Experiments with {A}da{B}oost{RT}, an improved boosting scheme for regression},
	volume = {8},
	year = {2006}}

@article{Deng-Jin-Zhong-05,
	author = {Yu-Feng Deng and Xing Jin and Yi-Xin Zhong},
	doi = {10.1109/ICMLC.2005.1527553},
	journal = {Gastroenterology},
	masid = {50424578},
	owner = {LVU},
	timestamp = {2012.12.18},
	title = {Ensemble SVR for prediction of time series},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/ICMLC.2005.1527553}}

@inproceedings{Domingo-Watanabe-2000,
	address = {San Francisco},
	author = {C. Domingo and O. Watanabe},
	booktitle = {Proceedings of the Thirteenth Annual Conference on Computational Learning Theory},
	owner = {LVU},
	pages = {180-189},
	publisher = {Morgan Kaufmann},
	timestamp = {2012.09.30},
	title = {Mada{B}oost: {A} modification of {A}da{B}oost},
	year = {2000}}

@inproceedings{Drucker-97,
	address = {San Francisco, CA, USA},
	author = {H. Drucker},
	booktitle = {Proc. of the 14th Intenational Conferences on Machine Learning},
	owner = {LVU},
	pages = {107-115},
	publisher = {Morgan Kaufmann},
	timestamp = {2013.01.08},
	title = {Improving regressor using boosting techniques},
	year = {1997}}

@article{Elattar-Goulermas-Wu-10,
	author = {E.E. Elattar and J. Goulermas and Q.H. Wu},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics---Part C},
	number = {4},
	owner = {LVU},
	pages = {438-447},
	timestamp = {2012.12.23},
	title = {Electric load forecasting based on locally weighted support vector regression},
	volume = {40},
	year = {2010}}

@inproceedings{Esposito-Saitta2003,
	author = {R. Esposito and L. Saitta},
	booktitle = {Procedings of IJCAI'03},
	owner = {LVU},
	pages = {499 - 504},
	timestamp = {2011.05.09},
	title = {Monte {C}arlo theory as an explanation of bagging and boosting},
	year = {2003}}

@incollection{Ferreira-Figueiredo-2012,
	address = {New York},
	author = {A.J. Ferreira and M.A.T. Figueiredo},
	booktitle = {Ensemble Machine Learning: Methods and Applications},
	editor = {C. Zhang and Y. Ma},
	owner = {LVU},
	pages = {35-85},
	publisher = {Springer},
	timestamp = {2013.01.11},
	title = {Boosting algorithms: A review of methods, theory, and applications},
	year = {2012}}

@article{Freund-Shapire-97,
	author = {Y. Freund and R.E. Schapire},
	journal = {Journal of Computer and System Sciences},
	number = {1},
	owner = {LVU},
	pages = {119-139},
	timestamp = {2013.01.11},
	title = {A decision theoretic generalization of on-line learning and an application to boosting},
	volume = {55},
	year = {1997}}

@article{Gao-Gao-2010,
	author = {Y. Gao and F. Gao},
	journal = {Neurocomputing},
	number = {16-18},
	owner = {LVU},
	pages = {3079-3088},
	timestamp = {2013.01.11},
	title = {Edited AdaBoost by weighted kNN},
	volume = {73},
	year = {2010}}

@article{Guo-Viktor-2004,
	author = {H. Guo and H.L. Viktor},
	journal = {ACM SIGKDD Explorations Newsletter},
	owner = {lvu},
	pages = {30-39},
	timestamp = {2012.07.26},
	title = {Learning from imbalanced data sets with boosting and data generation: the {D}ata{B}oost-{IM} approach},
	volume = {6},
	year = {2004}}

@article{Ho-1998,
	author = {T.K. Ho},
	date-modified = {2019-07-23 21:26:35 +0300},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	number = {8},
	pages = {832--844},
	title = {The random subspace method for constructing decision forests},
	volume = {20},
	year = {1998}}

@inproceedings{Tian-Wang-Mao-09,
	address = {Shanghai, P.R. China},
	author = {Huixin Tian, Anna Wang, Zhizhong Mao},
	booktitle = {Proc. of the Joint 48th IEEE Conference on Decision and Control and 28th Chinese Control Conference},
	owner = {LVU},
	pages = {8375-8380},
	timestamp = {2013.01.08},
	title = {A new soft sensor modeling method based on modified {A}da{B}oost with incremental learning},
	year = {2009}}

@inproceedings{Jin-2003,
	address = {Washington DC},
	author = {R. Jin and Y. Liu and L. Si and J. Carbonell and A. Hauptmann},
	booktitle = {Proceedings of Twentieth International Conference on Machine Learning (ICML 03)},
	owner = {LVU},
	pages = {1-9},
	publisher = {AAAI Press},
	timestamp = {2012.09.30},
	title = {A new boosting algorithm using input-dependent regularizer},
	year = {2003}}

@incollection{Kegl-03,
	abstract = {Most boosting regression algorithms use the weighted average of base
	regressors as their final regressor. In this paper we analyze the
	choice of the weighted median. We propose a general boosting algorithm
	based on this approach. We prove boosting-type convergence of the
	algorithm and give clear conditions for the convergence of the robust
	training error. The algorithm recovers \textscAdaBoost and \textscAdaBoost
	as special cases. For boosting confidence-rated predictions, it leads
	to a new approach that outputs a different decision and interprets
	robustness in a different manner than the approach based on the weighted
	average. In the general, non-binary case we suggest practical strategies
	based on the analysis of the algorithm and experiments.},
	address = {Berlin Heidelberg},
	author = {B. Kegl},
	booktitle = {Learning Theory and Kernel Machines. Lecture Notes in Computer Science},
	owner = {LVU},
	pages = {258-272},
	publisher = {Springer},
	timestamp = {2012.12.23},
	title = {Robust regression by boosting the median},
	volume = {2777},
	year = {2003}}

@conference{Kim2002243,
	abbrev_source_title = {Proc. IEEE Int. Conf. Data Min. ICDM},
	address = {Maebashi},
	affiliation = {Department of Statistics, Ewha Womans University, Seoul, South Korea},
	author = {Kim, Y.},
	correspondence_address = {Kim, Y.; Department of Statistics, Ewha Womans University, Seoul, South Korea; email: ydkim@mm.ewha.ac.kr},
	document_type = {Conference Paper},
	isbn = {0769517544; 9780769517544},
	issn = {15504786},
	journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
	language = {English},
	note = {cited By (since 1996) 2; Conference of 2nd IEEE International Conference on Data Mining, ICDM '02; Conference Date: 9 December 2002 through 12 December 2002; Conference Code: 82449},
	owner = {LVU},
	pages = {243-249},
	references = {Bauer, E., Kohavi, R., An empirical comparison of voting classification algorithms: Bagging, Boosting and Variants (1999) Machine Learning, pp. 105-139; Breiman, L., Bagging predictors (1996) Machine Learning, pp. 123-140; Breiman, L., Arcing classifiers (1998) Annals of Statistics, pp. 801-846; Breiman, L., Random forests (2001) Machine Learning, pp. 5-32; B{\"u}hlmann, P., Yu, B., (2001) Boosting with the L2 Loss: Regression and Classification, , Technical Report; Dietterich, T.G., An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, Boosting and Randomization (2000) Machine Learning, pp. 139-157; Freund, Y., Schapire, R., A decision-theoretic generalization of on-line learning and an application to boosting (1997) Journal of Computer and System Sciences, pp. 119-139; Friedman, J.H., Multivariate adaptive regression splines (with discussion) (1991) It Annals of Statistics, pp. 1-141; Friedman, J.H., Greedy function approximation: A gradient boosting machine (2001) Annals of Statistics, pp. 1189-1232; Friedman, J.H., Hastie, T., Tibshirani, R., Additive logistic regression: A statistical view of boosting (2000) Annals of Statistics, pp. 337-374; Jiang, W., On weak base hypotheses and their implications for boosting regression classification (2002) Annals of Statistics, pp. 51-73; Kim, Y., Kim, J., Jan, W., (2002) Convex Hull Ensemble Machine, , http://home.ewha.ac.kr/ydkim, Unpublished manuscript at; Opitz, D., Maclin, R., Popular ensemble methods: An empirical study (1999) Journal of Artificial Intelligence Research, pp. 169-198; Quinlan, J., Boosting first-order learning LNAI, 1160, pp. 143-155. , S. Arikawa & Sharma Eds., Proceedings of the 7th International Workshop on Algorithmic Learning Theory Berlin : Springer; R{\"a}sch, G., Onoda, T., M{\"u}ller, K.R., Soft margins for AdaBoost (2001) Machine Learning, pp. 287-320; Ridgeway, G., Contribution to the discussion of paper by Friedman, Hastie and Tibshirani (2000) Ann. Statist., pp. 393-400; Schapire, R., Freund, Y., Bartlett, P., Lee, W., Boosting the margin: A new explanation for the effectiveness of voting methods (1998) Ann. Statist., pp. 1651-1686; Schapire, R., Singer, Y., Improved boosting algorithms using confidence-rated predictions (1999) Machine Learning, pp. 297-336},
	source = {Scopus},
	sponsors = {IEEE Comput. Soc. Tech. Comm. Pattern; Anal. Mach. Intell. (TCPAMI); IEEE Comput. Soc. Tech. Comm. Comput. Intell. (TCCI)},
	timestamp = {2012.11.17},
	title = {Convex hull ensemble machine},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-62649083059&partnerID=40&md5=8c089cb64c82c1747dd1cd3a20cb68c3},
	year = {2002},
	bdsk-url-1 = {http://www.scopus.com/inward/record.url?eid=2-s2.0-62649083059&partnerID=40&md5=8c089cb64c82c1747dd1cd3a20cb68c3}}

@article{Lorbert-2012,
	adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1209.1996L},
	archiveprefix = {arXiv},
	author = {A. Lorbert and D.~M. Blei and R.~E. Schapire and P.~J. Ramadge},
	eprint = {1209.1996},
	journal = {ArXiv e-prints},
	owner = {LVU},
	timestamp = {2013.01.11},
	title = {A {B}ayesian boosting model},
	year = {2012}}

@inproceedings{Lu-Tian-Huang-07,
	address = {Berlin, Heidelberg},
	author = {Y. Lu and Q. Tian and T. Huang},
	booktitle = {Proceedings of the 7th International Conference on Multiple Classifier Systems, MCS'07},
	owner = {LVU},
	pages = {180-189},
	publisher = {Springer-Verlag},
	timestamp = {2012.09.30},
	title = {Interactive boosting for image classification},
	year = {2007}}

@article{Mease-Wyner-2008,
	author = {D. Mease and A. Wyner},
	journal = {Journal of Machine Learning Research},
	owner = {LVU},
	pages = {131-156},
	timestamp = {2012.09.29},
	title = {Evidence contrary to the statistical view of boosting},
	volume = {9},
	year = {2008}}

@article{Moreira-2012,
	author = {J.M. Moreira and C. Soares and A.M. Jorge and J.F.de Sousa},
	journal = {ACM Computing Surveys},
	number = {1},
	owner = {LVU},
	pages = {1-40},
	timestamp = {2012.12.23},
	title = {Ensemble approaches for regression: A survey},
	volume = {45},
	year = {2012}}

@article{Duffy-Helmbold-02,
	abstract = {In this paper we examine ensemble methods for regression that leverage
	or ``boost'' base regressors by iteratively calling them on modified
	samples. The most successful leveraging algorithm for classification
	is AdaBoost, an algorithm that requires only modest assumptions on
	the base learning method for its strong theoretical guarantees. We
	present several gradient descent leveraging algorithms for regression
	and prove AdaBoost-style bounds on their sample errors using intuitive
	assumptions on the base learners. We bound the complexity of the
	regression functions produced in order to derive PAC-style bounds
	on their generalization errors. Experiments validate our theoretical
	results.},
	author = {N. Duffy, D. Helmbold},
	journal = {Machine Learning},
	number = {2-3},
	owner = {LVU},
	pages = {153-200},
	timestamp = {2012.12.23},
	title = {Boosting methods for regression},
	volume = {47},
	year = {2002}}

@article{Nakamura-2004,
	author = {M. Nakamura and H. Nomiya and K. Uehara},
	issue = {1},
	journal = {Annals of Mathematics and Artificial Intelligence},
	owner = {LVU},
	pages = {95-109},
	timestamp = {2013.01.11},
	title = {Improvement of boosting algorithm by modifying the weighting rule},
	volume = {41},
	year = {2004}}

@incollection{Panov-Dzeroski-2007,
	address = {Berlin Heidelberg},
	author = {P. Panov and S. Dzeroski},
	booktitle = {Advances in Intelligent Data Analysis VII},
	owner = {lev u},
	pages = {118-129},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {2015.10.10},
	title = {Combining bagging and random subspaces to create better ensembles},
	volume = {4723},
	year = {2007}}

@inproceedings{Pardoe-Stone-10,
	address = {Haifa, Israel},
	author = {D. Pardoe and P. Stone},
	booktitle = {Proceedings of the 27th International Conference on Machine Learning},
	owner = {LVU},
	pages = {863-870},
	timestamp = {2013.01.08},
	title = {Boosting for regression transfer},
	year = {2010}}

@article{Peng-2006,
	author = {Y. Peng},
	journal = {Computers in Biology and Medicine},
	owner = {LVU},
	pages = {553-573},
	timestamp = {2012.06.11},
	title = {A novel ensemble machine learning for robust microarray data classification},
	volume = {36},
	year = {2006}}

@incollection{Re-Valentini-2012,
	author = {M. Re and G. Valentini},
	booktitle = {Data Mining and Machine Learning for Astronomical Applications},
	chapter = {26},
	owner = {lvu},
	pages = {563-594},
	publisher = {Chapman \& Hall},
	series = {Data Mining and Knowledge Discovery Series},
	timestamp = {2012.07.27},
	title = {Ensemble methods: a review},
	year = {2012}}

@article{ReboiroJato-2013,
	abstract = {In the last years, microarray technology has become widely used in
	relevant biomedical areas such as drug target identification, pharmacogenomics
	or clinical research. However, the necessary prerequisites for the
	development of valuable translational microarray-based diagnostic
	tools are (i) a solid understanding of the relative strengths and
	weaknesses of underlying classification methods and (ii) a biologically
	plausible and understandable behaviour of such models from a biological
	point of view. In this paper we propose a novel classifier able to
	combine the advantages of ensemble approaches with the benefits obtained
	from the true integration of biological knowledge in the classification
	process of different microarray samples. The aim of the current work
	is to guarantee the robustness of the proposed classification model
	when applied to several microarray data in an inter-dataset scenario.
	The comparative experimental results demonstrated that our proposal
	working with biological knowledge outperforms other well-known simple
	classifiers and ensemble alternatives in binary and multiclass cancer
	prediction problems using publicly available data.},
	author = {M. Reboiro-Jato and R. Laza and H. L?pez-Fern?ndez and D. Glez-Pe?a and F. D?az and F. Fdez-Riverola},
	journal = {Expert Systems with Applications},
	number = {1},
	owner = {LVU},
	pages = {52-63},
	timestamp = {2013.01.11},
	title = {genEnsemble: A new model for the combination of classifiers and integration of biological knowledge applied to genomic data},
	volume = {40},
	year = {2013}}

@inproceedings{Ridgeway-Madigan-Richardson-99,
	address = {San Francisco, CA},
	author = {Ridgeway, G. and Madigan, D. and Richardson, T.},
	booktitle = {Proc. of the Seventh International Workshop on Artificial Intelligence and Statistics},
	pages = {152-161},
	publisher = {Morgan Kaufmann},
	title = {Boosting methodology for regression problems},
	year = {1999}}

@article{Rokach-2010,
	author = {L. Rokach},
	journal = {Artificial Intelligence Review},
	number = {1-2},
	owner = {LVU},
	pages = {1-39},
	timestamp = {2012.06.11},
	title = {Ensemble-based classifiers},
	volume = {33},
	year = {2010}}

@article{Borra-Ciaccio-02,
	abstract = {Recently, many authors have proposed new algorithms to improve the
	accuracy of certain classifiers by assembling a collection of individual
	classifiers obtained resampling on the training sample. Bagging and
	boosting are well-known methods in the machine learning context and
	they have been proved to be successful in classification problems.
	In the regression context, the application of these techniques has
	received little investigation. Our aim is to analyse, by simulation
	studies, when boosting and bagging can reduce the training set error
	and the generalization error, using nonparametric regression methods
	as predictors. In this work, we will consider three methods: projection
	pursuit regression (PPR), multivariate adaptive regression splines
	(MARS), local learning based on recursive covering (DART).},
	author = {S. Borra, A.Di Ciaccio},
	journal = {Computational Statistics \& Data Analysis},
	number = {4},
	owner = {LVU},
	pages = {407-420},
	timestamp = {2012.12.23},
	title = {Improving nonparametric regression methods by bagging and boosting},
	volume = {38},
	year = {2002}}

@article{Servedio-2003,
	author = {R.A. Servedio},
	journal = {Journal of Machine Learning Research},
	number = {9},
	owner = {LVU},
	pages = {633-648},
	timestamp = {2012.09.30},
	title = {Smooth boosting and learning with malicious noise},
	volume = {4},
	year = {2003}}

@article{Shrestha-Solomatine-2006,
	author = {D.L. Shrestha and D.P. Solomatine},
	journal = {Neural Computation},
	number = {7},
	owner = {LVU},
	pages = {1678-1710},
	timestamp = {2013.01.11},
	title = {Experiments with {A}da{B}oost.{RT}, an improved boosting scheme for regression},
	volume = {18},
	year = {2006}}

@inproceedings{Skurichina-Duin-2001,
	address = {Berlin Heidelberg},
	author = {M. Skurichina and R.P.W.Duin},
	booktitle = {Multiple Classifier Systems},
	pages = {1-10},
	publisher = {Springer},
	title = {Bagging and the random subspace method for redundant feature spaces},
	year = {2001}}

@inproceedings{Solomatine-Shrestha-04,
	address = {Budapest, Hungary},
	author = {D.P. Solomatine and D.L. Shrestha},
	booktitle = {Proc. of the International Joint Conference on Neural Networks},
	owner = {LVU},
	pages = {1163-1168},
	timestamp = {2013.01.08},
	title = {AdaBoost.RT: a boosting algorithm for regression problems},
	year = {2004}}

@article{Sun-2007,
	author = {Y. Sun and M.C. Kamel and A.K.C. Wong and Y. Wang},
	journal = {Pattern Recognition},
	number = {12},
	owner = {lvu},
	pages = {3358-3378.},
	timestamp = {2012.07.26},
	title = {Cost-sensitive boosting for classification of imbalanced data},
	volume = {40},
	year = {2007}}

@article{Wang2012549,
	author = {Wang, S.a and Lee, S.-J.b},
	coden = {CYSYD},
	date-modified = {2022-01-14 01:25:50 +0300},
	document_type = {Article},
	journal = {Cybernetics and Systems},
	language = {English},
	number = {7},
	owner = {LVU},
	pages = {549-566},
	references = {Bauer, E., Kohavi, R., An empirical comparison of voting classification algorithms: Bagging, boosting and variants (1999) Machine Learning, 36, pp. 105-139; Blake, C., Keogh, E., Merz, C.J., (1998) UCI Repository of Machine Learning Databases, , http://www.ics.uci.edu/~mlearn/MLRepository.html, (accessed 15 November 2011); Breiman, L., Arcing classifiers (1998) Annals of Statistics, 26 (3), pp. 801-849; Dietterich, T.G., An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization (2000) Machine Learning, 40 (2), pp. 139-157; Domingo, C., Watanabe, O., (2000) MadaBoost: A Modification of AdaBoost, , Paper presented at the 13th Conference on Computational Learning Theory, Stanford University, CA, July; Feng, X., (2004) Facial Expression Recognition Based on Local Binary Patterns and Coarse-to- Fine Classification, , Paper presented at the Fourth International Conference on Computer and Information Technology, Wuhan, China, September 14-16; Feng, X., Hadid, A., Pietikainen, M., A coarse-to-fine classification scheme for facial expression recognition (2004) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 3212, pp. 668-675; Freund, Y., Schapire, R.E., A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting (1997) Journal of Computer and System Sciences, 55 (1), pp. 119-139; Friedman, J., Hastie, T., Tibshirani, R., Additive logistic regression: A statistical view of boosting (2000) Annals of Statistics, 28 (2), pp. 337-407; Kearns, M., Valiant, L.G., (1988) Learning Boolean Formulae or Finite Automata Is As Hard As Factoring, , Cambridge, MA: Harvard University Aiken Computation Laboratory. Tech. Rep. TR-14-88; Kearns, M., Valiant, L.G., Cryptographic limitations on learning boolean formulae and finite automata (1994) Journal of the Association for Computing Machinery, 41 (1), pp. 67-95; Lecun, Y., Jackel, L., Bottou, L., Cortes, C., Denker, J., Drucker, H., Muller Guyon, I., Vapnik, V., Learning algorithms for classification: A comparison on handwritten digit recognition (1995) Neural Networks: The Statistical Mechanics Perspective, pp. 261-276; Mason, L., Baxter, J., Bartlett, P.L., Frean, M., (1999) Boosting Algorithms As Gradient Descent, , Advances in Neural Information Processing Systems; Opitz, D., Maclin, R., Popular ensemble methods: An empirical study (1999) Journal of Artificial Intelligence Research, 11, pp. 169-198; Quinlan, J.R., Boosting first-order learning (1996) Proceedings of the 7th International Workshop on Algorithmic Learning Theory, pp. 143-155. , edited by S. Arikawa and A. K. Sharma. Berlin: Springer; Ratsch, G., Onoda, T., Muller, K.-R., Soft margins for AdaBoost (2001) Machine Learning, 42 (3), pp. 287-320. , DOI 10.1023/A:1007618119488; Rifkin, R., Klautau, A., In defense of one-vs.-All classification (2004) Journal of Machine Learning Research, 5, pp. 101-141; Schapire, R., Freund, Y., Bartlett, P., Lee, W., Boosting the margin: A new explanation for the effectiveness of voting methods (1997) Proceedings ICML'97: International Conference on Machine Learning, 12, pp. 322-330. , Los Altos, CA: Morgan Kaufmann: 512-518; Schwenk, H., Bengio, Y., Adaboosting neural networks (1997) Proceedings ICANN'97: International Conference on Artificial Neural Networks, pp. 967-972. , edited by W. Gerstner, A. Germond, M. Hasler and J.-D. Nicoud. Berlin: Springer; Valiant, L.G., A theory of the learnable (1984) Communications of the ACM, 27 (11), pp. 1134-1142; Verbaeten, S., Van Assche, A., Ensemble methods for noise elimination in classification problems (2003) Multiple Classifier Systems: Fourth International Workshop, pp. 317-325. , edited by T. Windeatt and F. Roli; Weinberger, K.Q., Blitzer, J., Saul, L.K., Distance metric learning for large margin nearest neighbor classification (2005) Advances in Neural Information Processing Systems, 17, pp. 1473-1480; Whang, K.Y., Eon, J.J., Shim, K., Srivatava, J., (2003) Averaged Boosting: A Noise-Robust Ensemble Method, , Paper read at PAKDD},
	source = {Scopus},
	timestamp = {2012.11.17},
	title = {A robust boosting by using an adaptive weight scheme},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84867757906&partnerID=40&md5=dad60eccbafc2332e48f50452d4faa8c},
	volume = {43},
	year = {2012},
	bdsk-url-1 = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84867757906&partnerID=40&md5=dad60eccbafc2332e48f50452d4faa8c},
	bdsk-url-2 = {https://doi.org/10.1080/01969722.2012.717856}}

@incollection{Warmuth-Glocer-Ratsch-07,
	author = {M. Warmuth and K. Glocer and G. Ratsch},
	booktitle = {Advances in Neural Information Processing Systems NIPS},
	owner = {LVU},
	pages = {1-8},
	publisher = {MIT Press},
	timestamp = {2012.09.30},
	title = {Boosting algorithms for maximizing the soft margin pages},
	year = {2007}}

@inproceedings{He-Mao-Liu-11,
	abstract = {As a universal learning method, Support Vector Regression (SVR) has
	strong generalization ability and can perfectly solve some practical
	problems, such as small samples, nonlinear, high dimension and so
	on. However, the prediction accuracy of a single SVR is limited.
	With the help of ensemble learning, the sample prediction accuracy
	of SVR can be effectively improved. In ensemble learning, the construction
	method of training samples is a key. The larger difference between
	the training sample sub-sets leads to the stronger generalization
	ability of SVR. In this work, an improved method of sample construction
	is proposed to increase the differences between training sample sets.
	The proposed method divides the samples into several sub-categories
	by clustering algorithm. Each sub-category adds the samples closing
	to the clustering center from other sub-categories to form a new
	training sample set. The experiment results demonstrate that the
	proposed improved method has less number of iterations and higher
	predict accuracy as compared to the method with random sampling.},
	author = {Yaxuan He, Jingli Mao, Yong Liu},
	booktitle = {Advanced Intelligence and Awareness Internet (AIAI 2011)},
	owner = {LVU},
	pages = {184-188},
	timestamp = {2012.12.23},
	title = {An improved ensemble learning method based on SVR},
	year = {2011}}

@incollection{Zemel-Pitassi-01,
	address = {Cambridge, MA},
	author = {R.S. Zemel and T. Pitassi},
	booktitle = {Advances in neural information processing systems},
	editor = {T.K. Leen, T.G. Dietterich, V. Tresp},
	owner = {LVU},
	pages = {696-702},
	publisher = {MIT Press},
	timestamp = {2012.12.23},
	title = {A gradient-based boosting algorithm for regression problems},
	volume = {13},
	year = {2001}}

@article{Zhang-Yu-2005,
	author = {T. Zhang and B. Yu},
	journal = {Annals of Statistics},
	number = {4},
	owner = {LVU},
	pages = {1538-1579},
	timestamp = {2012.09.29},
	title = {Boosting with early stopping: Convergence and consistency},
	volume = {33},
	year = {2005}}

@article{Rokach-2016,
	author = {L. Rokach},
	journal = {Information Fusion},
	pages = {111--125},
	title = {Decision forest: Twenty years of research},
	volume = {27},
	year = {2016}}

@article{Wozniak-etal-2014,
	author = {M. Wozniak and M. Grana and E. Corchado},
	journal = {Information Fusion},
	owner = {lev u},
	pages = {3--17},
	timestamp = {2018.01.20},
	title = {A survey of multiple classifier systems as hybrid systems},
	year = {2014}}

@article{Ren-Zhang-Suganthan-2016,
	author = {Y. Ren and L. Zhang and P. N. Suganthan},
	journal = {IEEE Computational Intelligence Magazine},
	number = {1},
	pages = {41-53},
	title = {Ensemble Classification and Regression-Recent Developments, Applications and Future Directions [Review Article]},
	volume = {11},
	year = {2016}}

@article{Fawagreh-etal-2014,
	author = {K. Fawagreh and M.M. Gaber and E. Elyan},
	journal = {Systems Science \& Control Engineering},
	number = {1},
	pages = {602-609},
	title = {Random forests: from early developments to recent advancements},
	volume = {2},
	year = {2014}}

@article{Yang-Yang-etal-2010,
	author = {P. Yang and E.H. Yang and B.B. Zhou and A.Y. Zomaya},
	journal = {Current Bioinformatics},
	number = {4},
	owner = {lev u},
	pages = {296--308},
	timestamp = {2018.01.20},
	title = {A review of ensemble methods in bioinformatics},
	volume = {5},
	year = {2010}}

@book{ZH-Zhou-2012,
	address = {Boca Raton},
	author = {Z.-H. Zhou},
	owner = {lev u},
	publisher = {CRC Press},
	timestamp = {2018.01.20},
	title = {Ensemble Methods: Foundations and Algorithms},
	year = {2012}}

@article{Criminisi-etal-2011,
	author = {A. Criminisi and J. Shotton and E. Konukoglu},
	journal = {Foundations and Trends in Computer Graphics and Vision},
	number = {2-3},
	owner = {lev u},
	pages = {81--227},
	timestamp = {2018.01.20},
	title = {Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning},
	volume = {7},
	year = {2011}}

@unpublished{Louppe-2015,
	author = {G. Louppe},
	month = {June},
	note = {arXiv:1407.7502v3},
	owner = {lev u},
	timestamp = {2018.01.20},
	title = {Understanding Random Forests: From Theory to Practice},
	year = {2015}}

@unpublished{Biau-Scornet-2015,
	author = {G. Biau and E. Scornet},
	month = {Nov},
	note = {arXiv:1511.05741v1},
	owner = {lev u},
	timestamp = {2018.01.20},
	title = {A Random Forest Guided Tour},
	year = {2015}}

@article{Genuer-etal-2017,
	author = {R. Genuer and J.-M. Poggi and C. Tuleau-Malot and N. Villa-Vialaneix},
	journal = {Big Data Research},
	owner = {lev u},
	pages = {28--46},
	timestamp = {2018.01.20},
	title = {Random Forests for Big Data},
	volume = {9},
	year = {2017}}

@article{Jurek-etal-2014,
	author = {A. Jurek and Y. Bi and S. Wu and C. Nugent},
	journal = {The Knowledge Engineering Review},
	number = {5},
	owner = {lev u},
	pages = {551--581},
	timestamp = {2018.01.20},
	title = {A survey of commonly used ensemble-based classification techniques},
	volume = {29},
	year = {2014}}

@incollection{Polikar-2012,
	address = {New York},
	author = {R. Polikar},
	booktitle = {Ensemble Machine Learning: Methods and Applications},
	editor = {C. Zhang and Y. Ma},
	owner = {lev u},
	pages = {1--34},
	publisher = {Springer},
	timestamp = {2018.01.22},
	title = {Ensemble learning},
	year = {2012}}

@article{Feng-Dauphin-2019,
	author = {W. Feng and G. Dauphin and W. Huang and Y. Quan and W. Liao},
	journal = {Knowledge-Based Systems},
	number = {Article 104845},
	pages = {1-12},
	title = {New margin-based subsampling iterative technique in modified random forests for classification},
	volume = {182},
	year = {2019}}

@article{Nadi-Moradi-2019,
	author = {A. Nadi and H. Moradi},
	journal = {Expert Systems with Applications},
	number = {Article 112801},
	pages = {1-13},
	title = {Increasing the views and reducing the depth in random forest},
	volume = {138},
	year = {2019}}

@article{Badarna-Shimshoni-2019,
	author = {M. Badarna and I. Shimshoni},
	journal = {Neurocomputing},
	pages = {93--108},
	title = {Selective sampling for trees and forests},
	volume = {358},
	year = {2019}}

@article{Liu-Zhang-2019,
	author = {H. Liu and L. Zhang},
	journal = {Expert Systems with Applications},
	pages = {20--29},
	title = {Advancing Ensemble Learning Performance through data transformation and classifiers fusion in granular computing context},
	volume = {131},
	year = {2019}}

@article{Lodhi-Kang-2019,
	author = {B. Lodhi and J. Kang},
	journal = {Information Sciences},
	pages = {63--72},
	title = {Multipath-DenseNet: A Supervised ensemble architecture of densely connected convolutional networks},
	volume = {482},
	year = {2019}}

@article{Moral-Garcia-etal-2020,
	author = {S. Moral-Garcia and C.J. Mantas and J.G. Castellano and M.D. Benitez and J. Abellan},
	journal = {Expert Systems with Applications},
	number = {Article 112944},
	pages = {1--9},
	title = {Bagging of credal decision trees for imprecise classification},
	volume = {141},
	year = {2020}}

@article{Moral-2019,
	author = {S. Moral},
	journal = {International Journal of Approximate Reasoning},
	pages = {111--124},
	title = {Learning with imprecise probabilities as model selection and averaging},
	volume = {109},
	year = {2019}}

@article{Matt-2017,
	author = {P.-A. Matt},
	journal = {International Journal of Approximate Reasoning},
	pages = {63--86},
	title = {Uses and computation of imprecise probabilities from statistical data and expert arguments},
	volume = {81},
	year = {2017}}

@article{Friedman-2002,
	author = {J.H. Friedman},
	journal = {Computational statistics \& data analysis},
	number = {4},
	pages = {367--378},
	title = {Stochastic gradient boosting},
	volume = {38},
	year = {2002}}

@article{Natekin-Knoll-13,
	author = {A. Natekin and A. Knoll},
	journal = {Frontiers in neurorobotics},
	number = {Article 21},
	pages = {1--21},
	title = {Gradient boosting machines, a tutorial},
	volume = {7},
	year = {2013}}

@article{Sagi-Rokach-2018,
	author = {O. Sagi and L. Rokach},
	journal = {WIREs Data Mining and Knowledge Discovery},
	number = {e1249},
	pages = {1--18},
	title = {Ensemble learning: A survey},
	volume = {8},
	year = {2018}}

@article{Friedman-2001,
	author = {J.H. Friedman},
	journal = {Annals of Statistics},
	pages = {1189--1232},
	title = {Greedy function approximation: A gradient boosting machine},
	volume = {29},
	year = {2001}}

@inproceedings{Chen-Guestrin-2016,
	address = {New York, NY},
	author = {T. Chen and C. Guestrin},
	booktitle = {Proceedings of the 22nd {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {785--794},
	publisher = {ACM},
	title = {Xgboost: A scalable tree boosting system},
	year = {2016}}

@unpublished{Dorogush-etal-2018,
	author = {A.V. Dorogush and V. Ershov and A. Gulin},
	month = oct,
	note = {arXiv:1810.11363},
	title = {CatBoost: gradient boosting with categorical features support},
	year = {2018}}

@article{Breiman-1996a,
	author = {L. Breiman},
	journal = {Machine Learning},
	number = {1},
	pages = {49--64},
	title = {Stacked regressions},
	volume = {24},
	year = {1996}}

@article{Sesmero-etal-15,
	author = {M.P. Sesmero and A.I. Ledezma and A. Sanchis},
	journal = {WIREs Data Mining and Knowledge Discovery},
	pages = {21--34},
	title = {Generating ensembles of heterogeneous classifiers using Stacked Generalization},
	volume = {5},
	year = {2015}}

@unpublished{Bilal-2019,
	author = {E. Bilal},
	month = oct,
	note = {arXiv:1907.12608},
	title = {Deep Gradient Boosting -- Layer-wise Input Normalization of Neural Networks},
	year = {2019}}

@unpublished{Badirli-etal-2020,
	author = {S. Badirli and X. Liu and Z. Xing and A. Bhowmik and S.S. Keerthi},
	month = feb,
	note = {arXiv:2002.07971},
	title = {Gradient Boosting Neural Networks: GrowNet},
	year = {2020}}

@article{Weldegebriel-etal-2019,
	author = {H.T. Weldegebriel and H. Liu and A.U. Haq and E. Bugingo and D. Zhang},
	journal = {IEEE Access},
	pages = {17804--17818.},
	title = {A New Hybrid Convolutional Neural Network and eXtreme Gradient Boosting Classifier for Recognizing Handwritten Ethiopian Characters},
	volume = {8},
	year = {2019}}

@inproceedings{Bengio-etal-2006,
	author = {Y. Bengio and N.L. Roux and P. Vincent and O. Delalleau and P. Marcotte},
	booktitle = {Advances in neural information processing systems},
	pages = {123--130},
	title = {Convex neural networks},
	year = {2006}}

@unpublished{Nitanda-Suzuki-2018,
	author = {A. Nitanda and T. Suzuki},
	month = feb,
	note = {arXiv:1802.09031},
	title = {Functional gradient boosting based on residual network perception},
	year = {2018}}

@unpublished{Lu-Mazumder-18,
	author = {H. Lu and R. Mazumder},
	month = oct,
	note = {arXiv:1810.10158v2},
	title = {Randomized Gradient Boosting Machine},
	year = {2018}}

@unpublished{Lu-etal-2019,
	author = {H. Lu and S.P. Karimireddy and N. Ponomareva and V. Mirrokni},
	month = sep,
	note = {arXiv:1903.08708v2},
	title = {Accelerating Gradient Boosting Machine},
	year = {2019}}

@inproceedings{Wehenkel-etal-06,
	author = {L. Wehenkel and D. Ernst and P. Geurts},
	booktitle = {Proceedings of Robust Methods for Power System State Estimation and Load Forecasting},
	pages = {1--10},
	title = {Ensembles of extremely randomized trees and some generic applications},
	url = {http://hdl.handle.net/2268/13447},
	year = {2006},
	bdsk-url-1 = {http://hdl.handle.net/2268/13447}}

@article{Dietterich-00,
	author = {T.G. Dietterich},
	journal = {Machine Learning},
	number = {2},
	pages = {139--157},
	title = {An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization},
	volume = {40},
	year = {2000}}

@inproceedings{Guolin-etal-17,
	author = {K. Guolin and M. Qi and F. Thomas and W. Taifeng and C. Wei and M. Weidong and Y. Qiwei and L. Tie-Yan},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17))},
	pages = {3149--3157},
	title = {LightGBM: A highly efficient gradient boosting decision tree},
	year = {2017}}

@inproceedings{Guryanov-19,
	address = {Cham},
	author = {A. Guryanov},
	booktitle = {Proceedings of the 8th International Conference on Analysis of Images, Social Networks and Texts. AIST 2019},
	pages = {39--50},
	publisher = {Springer},
	series = {LNCS},
	title = {Histogram-Based Algorithm for Building Gradient Boosting Ensemblesof Piecewise Linear Decision Trees},
	volume = {11832},
	year = {2019}}

@article{Buhlmann-Hothorn-07,
	author = {P. Buhlmann and T. Hothorn},
	journal = {Statistical Science},
	number = {4},
	pages = {477--505},
	title = {Boosting Algorithms: Regularization, Prediction and Model Fitting},
	volume = {22},
	year = {2007}}
