% Encoding: UTF-8




@inproceedings{Mass-etal-2011,
	Author = {A.L. Maas and R.E. Daly and P.T. Pham and D. Huang and A.Y. Ng and C. Potts},
	Booktitle = {Proceedings of the 49th annual meeting of the association for computational linguistics: {H}uman language technologies},
	Date-Added = {2019-01-02 11:05:40 +0300},
	Date-Modified = {2019-07-19 13:54:54 +0300},
	Pages = {142--150},
	Publisher = {Association for Computational Linguistics},
	Title = {Learning Word Vectors for Sentiment Analysis},
	Volume = {1},
	Year = {2011}}

@misc{Dua:2017,
	Author = {Dheeru, Dua and Karra Taniskidou, Efi},
	Date-Added = {2019-01-01 23:19:52 +0300},
	Date-Modified = {2019-01-01 23:19:52 +0300},
	Institution = {University of California, Irvine, School of Information and Computer Sciences},
	Title = {{UCI} Machine Learning Repository},
	Url = {http://archive.ics.uci.edu/ml},
	Year = {2017},
	Bdsk-Url-1 = {http://archive.ics.uci.edu/ml}}

@techreport{Krizhevsky-Hinton-2009,
	Author = {A. Krizhevsky and G. Hinton},
	Date-Added = {2018-09-26 09:37:17 +0300},
	Date-Modified = {2018-09-26 09:38:49 +0300},
	Institution = {Computer Science Department, University of Toronto},
	Number = {1},
	Title = {Learning multiple layers of features from tiny images},
	Year = {2009}}

@article{Abraham2009,
	Abstract = {We propose a method for computing the range of the optimal decisions
	when the utility function runs through a class . The class has constraints
	on the values and the shape of the utility functions. A discretization
	method enables to easily approximate the optimal decision associated
	with a particular utility function . The range of optimal decisions
	is computed by a Monte Carlo optimization method. An example is provided
	with numerical results.},
	Author = {C. Abraham},
	Journal = {International Journal of Approximate Reasoning},
	Number = {2},
	Pages = {289 - 302},
	Title = {A computation method in robust Bayesian decision theory},
	Volume = {50},
	Year = {2009}}

@incollection{Alaiz-Rodriguez2009,
	Abstract = {The fundamental assumption that training and operational data come
	from the same probability distribution, which is the basis of most
	learning algorithms, is often not satisfied in practice. Several
	algorithms have been proposed to cope with classification problems
	where the class priors may change after training, but they can show
	a poor performance when the class conditional data densities also
	change. In this paper, we propose a re-estimation algorithm that
	makes use of unlabeled operational data to adapt the classifier behavior
	to changing scenarios. We assume that (a) the classes may be decomposed
	in several (unknown) subclasses, and (b) the prior subclass probabilities
	may change after training. Experimental results with practical applications
	show an improvement over an adaptive method based on class priors,
	while preserving a similar performance when there are no subclass
	changes.},
	Author = {R. Alaiz-Rodr\'{\i}guez and A. Guerrero-Curieses and J. Cid-Sueiro},
	Booktitle = {Bio-Inspired Systems: Computational and Ambient Intelligence},
	Editor = {J. Cabestany and F. Sandoval and A. Prieto and J. Corchado},
	Pages = {122-130},
	Publisher = {Springer Berlin / Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Improving Classification under Changes in Class and Within-Class Distributions},
	Volume = {5517},
	Year = {2009}}

@article{Alaiz-Rodriguez2007,
	Author = {R. Alaiz-Rodr\'{\i}guez and A. Guerrero-Curieses and J. Cid-Sueiro},
	Journal = {J. Mach. Learn. Res.},
	Pages = {103--130},
	Title = {Minimax Regret Classifier for Imprecise Class Distributions},
	Volume = {8},
	Year = {2007}}

@inproceedings{Angulo2007,
	Address = {Bruges, Belgium},
	Author = {C. Angulo and D. Anguita and L. Gonzalez},
	Booktitle = {11th European Symposium on Artificial Neural Networks},
	Owner = {LVU},
	Pages = {223-228},
	Timestamp = {2010.12.11},
	Title = {Interval discriminant analysis using support vector machines},
	Year = {2007}}

@article{Angulo2008,
	Author = {C. Angulo and D. Anguita and L. Gonzalez-Abril and J.A. Ortega},
	Journal = {Neurocomputing},
	Number = {7-9},
	Pages = {1220 - 1229},
	Title = {Support vector machines for interval discriminant analysis},
	Volume = {71},
	Year = {2008}}

@article{Auria-Moro2008,
	Author = {L. Auria and R.A. Moro},
	Journal = {DIW Berlin Discussion Paper No. 811},
	Title = {Support vector machines ({SVM}) as a technique for solvency analysis},
	Url = {http://ssrn.com/abstract=1424949},
	Year = {2008},
	Bdsk-Url-1 = {http://ssrn.com/abstract=1424949}}

@article{Bartlett-Wegkamp-2008,
	Author = {P.L. Bartlett and M.H. Wegkamp},
	Journal = {Journal of Machine Learning Research},
	Pages = {1823-1840},
	Title = {Classification with a reject option using a hinge loss},
	Volume = {9},
	Year = {2008}}

@book{Ben-Tal-2009,
	Address = {Princeton, New Jersey},
	Author = {A. Ben-Tal and L.E. Ghaoui and A. Nemirovski},
	Owner = {LVU},
	Publisher = {Princeton University Press},
	Timestamp = {2012.02.24},
	Title = {Robust optimization},
	Year = {2009}}

@article{Bennett-Campbell2000,
	Address = {New York, NY, USA},
	Author = {K.P. Bennett and C. Campbell},
	Issue = {2},
	Journal = {ACM SIGKDD Explorations Newsletter},
	Pages = {1--13},
	Publisher = {ACM},
	Title = {Support vector machines: hype or hallelujah?},
	Volume = {2},
	Year = {2000}}

@article{Berger-Salinetti1995,
	Abstract = {Solving Bayesian decision problems usually requires approximation
	procedures, all leading to study the convergence of the approximating
	infima. This aspect is analysed in the context of epigraphical convergence
	of integral functionals, as minimal context for convergence of infima.
	The results, applied to the Monte Carlo importance sampling, give
	a necessary and sufficient condition for convergence of the approximations
	of Bayes decision problems and sufficient conditions for a large
	class of Bayesian statistical decision problems.},
	Author = {J.O. Berger and G. Salinetti},
	Issue = {1},
	Journal = {Annals of Operations Research},
	Pages = {1-13},
	Title = {Approximations of {B}ayes decision problems: the epigraphical approach},
	Volume = {56},
	Year = {1995}}

@incollection{Bi-Zhang-2004,
	Address = {Cambridge, MA},
	Author = {J. Bi and T. Zhang},
	Booktitle = {Advances in Neural Information Processing Systems},
	Editor = {L.K. Saul and Y. Weiss and L. Bottou},
	Owner = {LVU},
	Pages = {161-168},
	Publisher = {MIT Press},
	Timestamp = {2012.02.25},
	Title = {Support vector classification with input data uncertainty},
	Volume = {17},
	Year = {2004}}

@article{Boulesteix-2008,
	Author = {Boulesteix A.-L., Strobl C., Augustin T., Daumer M.},
	Journal = {Cancer Informatics},
	Owner = {LVU},
	Pages = {77-97},
	Timestamp = {2012.06.12},
	Title = {Evaluating microarray-based classifiers: An overview},
	Volume = {6},
	Year = {2008}}

@article{Bouveyron2009,
	Abstract = {In the supervised classification framework, human supervision is required
	for labeling a set of learning data which are then used for building
	the classifier. However, in many applications, human supervision
	is either imprecise, difficult or expensive. In this paper, the problem
	of learning a supervised multi-class classifier from data with uncertain
	labels is considered and a model-based classification method is proposed
	to solve it. The idea of the proposed method is to confront an unsupervised
	modeling of the data with the supervised information carried by the
	labels of the learning data in order to detect inconsistencies. The
	method is able afterward to build a robust classifier taking into
	account the detected inconsistencies into the labels. Experiments
	on artificial and real data are provided to highlight the main features
	of the proposed method as well as an application to object recognition
	under weak supervision.},
	Author = {C. Bouveyron and S. Girard},
	Journal = {Pattern Recognition},
	Number = {11},
	Pages = {2649 - 2658},
	Title = {Robust supervised classification with mixture models: Learning from data with uncertain labels},
	Volume = {42},
	Year = {2009}}

@article{Breiman1996,
	Author = {L. Breiman},
	Journal = {Machine Learning},
	Number = {2},
	Owner = {LVU},
	Pages = {123-140},
	Timestamp = {2012.06.11},
	Title = {Bagging predictors},
	Volume = {24},
	Year = {1996}}

@inproceedings{Cerioli-Riani-Atkinson-2006,
	Author = {A. Cerioli and M. Riani and A.C. Atkinson},
	Booktitle = {Compstat 2006 - Proceedings in Computational Statistics},
	Editor = {A. Rizzi and M. Vichi},
	Owner = {LVU},
	Pages = {507-519},
	Publisher = {Physica-Verlag HD},
	Timestamp = {2012.02.20},
	Title = {Robust classification with categorical variables},
	Year = {2006}}

@manual{CC01a,
	Author = {C.-C. Chang and C.-J. Lin},
	Note = {Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm},
	Title = {{LIBSVM}: a library for support vector machines},
	Year = {2001}}

@incollection{Chapelle-Scholkopf-01,
	Address = {Cambridge, MA, USA},
	Author = {O. Chapelle and B. Scholkopf},
	Booktitle = {Advances in Neural Information Processing Systems},
	Editor = {T.G. Dietterich and S. Becker and Z. Ghahraman},
	Owner = {LVU},
	Pages = {609-616},
	Publisher = {MIT Press},
	Timestamp = {2012.10.28},
	Title = {Incorporating invariances in non-linear support vector machines},
	Year = {2001}}

@article{Chawla-2002,
	Author = {N.V. Chawla and K. Bowyer and L. Hall and W.P. Kegelmeyer},
	Journal = {Artificial Intelligence Research},
	Owner = {lvu},
	Pages = {321-357},
	Timestamp = {2012.07.26},
	Title = {{SMOTE}: Synthetic minority over-sampling technique},
	Volume = {16},
	Year = {2002}}

@book{Cherkassky-Mulier2007,
	Address = {UK},
	Author = {V. Cherkassky and F.M. Mulier},
	Pages = {538},
	Publisher = {Wiley-IEEE Press},
	Title = {Learning from Data: Concepts, Theory, and Methods},
	Year = {2007}}

@article{Christmann-Steinwart-2004,
	Author = {A. Christmann and I. Steinwart},
	Journal = {J. Mach. Learn. Res.},
	Month = {December},
	Pages = {1007--1034},
	Title = {On robustness properties of convex risk minimization methods for pattern recognition},
	Volume = {5},
	Year = {2004}}

@article{Denoeux2009,
	Author = {E. Come and L. Oukhellou and T. Denoeux and P. Aknin},
	Journal = {Pattern Recognition},
	Number = {3},
	Owner = {LVU},
	Pages = {334-348},
	Timestamp = {2011.01.28},
	Title = {Learning from partially supervised data using mixture models and belief functions},
	Volume = {42},
	Year = {2009}}

@article{Corani-Zaffalon08,
	Author = {G. Corani and M. Zaffalon},
	Journal = {Journal of Machine Learning Research},
	Owner = {LVU},
	Pages = {581--621},
	Timestamp = {2010.10.03},
	Title = {Learning reliable classifiers from small or incomplete data sets: the naive credal classifier 2.},
	Volume = {9},
	Year = {2008}}

@article{Dara-Kamel-Wanas-09,
	Abstract = {In this paper, the data dependency of aggregation modules in multiple
	classifier system is being investigated. We first propose a new categorization
	scheme, in which combining methods are grouped into data-independent,
	implicitly data-dependent and explicitly data-dependent. It is argued
	that data-dependent approaches present the highest potential for
	improved performance. In this study, we intend to provide a comprehensive
	investigation of this argument and explore the impact of data dependency
	on the performance of multiple classifiers. We evaluate this impact
	based on two criteria, prediction accuracy and stability. In addition,
	we examine the effect of class imbalance and uneven data distribution
	on these two criteria. This paper presents the findings of an extensive
	set of comparative experiments. Based on the findings, it can be
	concluded that data-dependent aggregation methods are generally more
	stable and less sensitive to class imbalance. In addition, data-dependent
	methods exhibited superior or identical generalization ability for
	most of the data sets.},
	Author = {R.A. Dara and M.S. Kamel and N. Wanas},
	Journal = {Pattern Recognition},
	Number = {7},
	Pages = {1260 - 1273},
	Title = {Data dependency in multiple classifier systems},
	Volume = {42},
	Year = {2009}}

@inproceedings{Dayanik-06,
	Address = {New York, NY, USA},
	Author = {A. Dayanik and D.D. Lewis and D. Madigan and V. Menkov and A. Genkin},
	Booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
	Pages = {493--500},
	Publisher = {ACM},
	Title = {Constructing informative prior distributions from domain knowledge in text classification},
	Year = {2006}}

@article{Decoste-02,
	Author = {Decoste, Dennis and Sch\"{o}lkopf, Bernhard},
	Issue_Date = {2002},
	Journal = {Machine Learning},
	Number = {1-3},
	Pages = {161-190},
	Title = {Training invariant support vector machines},
	Volume = {46},
	Year = {2002}}

@article{Demsar-2006,
	Author = {J. Demsar},
	Journal = {Journal of Machine Learning Research},
	Owner = {LVU},
	Pages = {1-30},
	Timestamp = {2013.12.19},
	Title = {Statistical comparisons of classifiers over multiple data sets},
	Volume = {7},
	Year = {2006}}

@article{Denoeux-Smets2006,
	Author = {T. Denoeux and P. Smets},
	Journal = {IEEE Transactions on Systems, Man and Cybernetics B},
	Number = {6},
	Owner = {LVU},
	Pages = {1395-1406},
	Timestamp = {2011.01.28},
	Title = {Classification using belief functions: the relationship between the case-based and model-based approaches},
	Volume = {36},
	Year = {2006}}

@article{Dinkelbach-1967,
	Author = {W. Dinkelbach},
	Journal = {Management Science},
	Owner = {LVU},
	Pages = {492--498},
	Timestamp = {2011.02.13},
	Title = {On nonlinear fractional programming},
	Volume = {13},
	Year = {1967}}

@article{Domingues-Souza-Cysneiros-2010,
	Abstract = {This paper introduces a new linear regression method for interval
	valued-data. The method is based on the symmetrical linear regression
	methodology such that the prediction of the lower and upper bounds
	of the interval value of the dependent variable is not damaged by
	the presence of interval-valued data outliers. The method considers
	mid-points and ranges of the interval values assumed by the variables
	in the learning set. The prediction of the boundaries of an interval
	is accomplished through a combination of predictions from mid-point
	and range of the interval values. The evaluation of the method is
	based on the average behavior of a pooled root mean-square error.
	Experiments with real and simulated symbolic interval data sets demonstrate
	the usefulness of this symbolic symmetrical linear regression method.},
	Author = {M.A.O. Domingues and R.M.C.R. de Souza and F.J.A. Cysneiros},
	Journal = {Pattern Recognition Letters},
	Number = {13},
	Pages = {1991 - 1996},
	Title = {A robust method for linear regression of symbolic interval data},
	Volume = {31},
	Year = {2010}}

@book{Duda-Hart73,
	Author = {R.O. Duda and P.E. Hart},
	Owner = {LVU},
	Publisher = {John Wiley \& Sons},
	Timestamp = {2010.10.03},
	Title = {Pattern classification and scene analysis},
	Year = {1973}}

@article{Evgeniou-2002,
	Author = {T. Evgeniou and T. Poggio and M. Pontil and A. Verri},
	Journal = {Computational Statistics \& Data Analysis},
	Number = {4},
	Pages = {421 - 432},
	Title = {Regularization and statistical learning theory for data analysis},
	Volume = {38},
	Year = {2002}}

@techreport{Ferson-Ginzburg-Akcakaya96,
	Author = {S. Ferson and L. Ginzburg and R. Akcakaya},
	Institution = {Applied Biomathematics Report},
	Journal = {Applied Biomathematics Report},
	Note = {http://www.ramas.com/whereof.pdf},
	Owner = {LVU},
	Timestamp = {2010.10.04},
	Title = {Whereof one cannot speak: When input distributions are unknown},
	Year = {2001}}

@misc{Frank+Asuncion:2010,
	Author = {A. Frank and A. Asuncion},
	Institution = {University of California, Irvine, School of Information and Computer Sciences},
	Title = {{UCI} Machine Learning Repository},
	Url = {http://archive.ics.uci.edu/ml},
	Year = {2010},
	Bdsk-Url-1 = {http://archive.ics.uci.edu/ml}}

@incollection{Fung-2002,
	Address = {Cambridge, MA, USA},
	Author = {G.M. Fung and O.L. Mangasarian and J.W. Shavlik},
	Booktitle = {Advances in Neural Information Processing Systems},
	Editor = {S. Becker and S. Thrun and K. Obermayer},
	Journal = {Advances in Neural Information Processing Systems},
	Owner = {LVU},
	Pages = {521-528},
	Publisher = {MIT Press},
	Timestamp = {2012.10.27},
	Title = {Knowledge-based support vector machine classifiers},
	Year = {2002}}

@article{Gartner-Lloyd-Flach-2004,
	Author = {T. Gartner and J.W. Lloyd and P.A. Flach},
	Journal = {Machine Learning},
	Number = {3},
	Owner = {lev u},
	Pages = {205--232},
	Timestamp = {2017.07.20},
	Title = {Kernels and Distances for Structured Data},
	Volume = {57},
	Year = {2004}}

@article{Golub-1999,
	Author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
	Journal = {Science},
	Number = {5439},
	Pages = {531-537},
	Title = {Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring},
	Volume = {286},
	Year = {1999}}

@article{Guo-Shi-2011,
	Abstract = {We study learning algorithms for classification generated by regularization
	schemes in reproducing kernel Hilbert spaces associated with a general
	convex loss function in a non-i.i.d. process. Error analysis is studied
	and our main purpose is to provide an elaborate capacity dependent
	error bounds by applying concentration techniques involving the l2-empirical
	covering numbers.},
	Author = {Z.-C. Guo and L. Shi},
	Journal = {Mathematical and Computer Modelling},
	Number = {5-6},
	Pages = {1347 - 1364},
	Title = {Classification with non-i.i.d. sampling},
	Volume = {54},
	Year = {2011}}

@incollection{Guyon-Stork00,
	Author = {I. Guyon and D.G. Stork},
	Booktitle = {Advances in large margin classifiers},
	Chapter = {7},
	Editor = {Smola and Bartlett and Schoelkopf and Schuurmans},
	Pages = {147-169},
	Title = {Linear discriminant and suppost vector classifiers},
	Year = {1999}}

@incollection{Haasdonk-05,
	Author = {Haasdonk, B. and Vossen, A. and Burkhardt, H.},
	Booktitle = {Image Analysis},
	Editor = {Kalviainen, H. and Parkkinen, J. and Kaarna, A.},
	Owner = {LVU},
	Pages = {841-851},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Timestamp = {2012.10.28},
	Title = {Invariance in kernel methods by haar-integration kernels},
	Volume = {3540},
	Year = {2005}}

@article{WEKA-2009,
	Author = {M. Hall and E. Frank and G. Holmes and B. Pfahringer and P. Reutemann and I.H. Witten},
	Journal = {SIGKDD Explorations},
	Number = {1},
	Owner = {LVU},
	Pages = {10--18},
	Timestamp = {2011.07.12},
	Title = {The {WEKA} data mining software: An update},
	Volume = {11},
	Year = {2009}}

@article{Hansen-Liisberg-Salamon-1997,
	Author = {L.K. Hansen and C. Liisberg and P. Salamon},
	Issue = {2},
	Journal = {Open Systems \&amp; Information Dynamics},
	Pages = {159-184},
	Publisher = {Springer Netherlands},
	Title = {The error-reject tradeoff},
	Volume = {4},
	Year = {1997}}

@article{Herbei-Wegkamp-2006,
	Author = {R. Herbei and M.H. Wegkamp},
	Journal = {Canadian Journal of Statistics},
	Number = {4},
	Pages = {709 - 721},
	Title = {Classification with reject option},
	Volume = {34},
	Year = {2006}}

@book{Hosmer-Lemeshow00,
	Address = {New York},
	Author = {D.W. Hosmer and S. Lemeshow},
	Editor = {Second Edition},
	Pages = {375},
	Publisher = {Wiley},
	Title = {Applied logistic regression},
	Year = {2000}}

@techreport{Cwh03a,
	Author = {Chih-Wei Hsu and Chih-Chung Chang and Chih-Jen Lin},
	Institution = {Department of Computer Science, National Taiwan University},
	Title = {A practical guide to support vector classification},
	Url = {http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf},
	Year = {2003},
	Bdsk-Url-1 = {http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf}}

@article{Hurtado2007,
	Abstract = {In structural reliability, simulation methods are oriented to the
	estimation of the probability integral over the failure domain, while
	solver-surrogate methods are intended to approximate such a domain
	before carrying out the simulation. A method combining these two
	purposes at a time and intended to obtain a drastic reduction of
	the computational labor implied by simulation techniques is proposed.
	The method is based on the concept that linear or nonlinear transformations
	of the performance function that do not affect the boundary between
	safe and failure classes lead to the same failure probability than
	the original function. Useful transformations that imply reducing
	the number of performance function calls can be built with several
	kinds of squashing functions. A most practical of them is provided
	by the pattern recognition technique known as support vector machines.
	An algorithm for estimating the failure probability combining this
	method with importance sampling is developed. The method takes advantage
	of the guidance offered by the main principles of each of these techniques
	to assist the other. The illustrative examples show that the method
	is very powerful. For instance, a classical series problem solved
	with O(1000) importance sampling solver calls by several authors
	is solved in this paper with less than 40 calls with similar accuracy.},
	Author = {J.E. Hurtado},
	Journal = {Structural Safety},
	Number = {1},
	Pages = {2 - 15},
	Title = {Filtered importance sampling with support vector margin: A powerful method for structural reliability analysis},
	Volume = {29},
	Year = {2007}}

@article{Hurtado2004,
	Abstract = {The reliability analysis of complex structures is hindered by the
	implicit nature of the limit-state function. For their approximation
	use has been made of the Response Surface Method (RSM) and, more
	recently, of Neural Networks. From the statistical viewpoint this
	corresponds to a regression approach. In the structural reliability
	literature little attention has been paid, however, to the possibility
	of treating the problem as a classification task. This enlarges the
	list of methods that are eventually useful to the purpose at hand
	and justifies an overall examination of their distinguishing features.
	This task is performed in this paper from the point of view of the
	Theory of Statistical Learning, which provides a unified framework
	for all regression, classification and probability density estimation.
	The classification methods are grouped into three categories and
	it is shown that only one group is useful for structural reliability,
	according to some specific criteria. In this category are the Multi-Layer
	Perceptrons and the Support Vector Machines, which are the recommended
	methods because (a) they can estimate the function on the basis of
	a few samples, (b) they use flexible and adaptive models and (c)
	they can overcome the curse of dimensionality. The paper also includes
	an in-depth analysis of the RSM from the point of view of statistical
	learning. It is shown that the empirically found instability of this
	method is explained with statistical learning concepts.},
	Author = {J.E. Hurtado},
	Journal = {Structural Safety},
	Number = {3},
	Pages = {271 - 293},
	Title = {An examination of methods for approximating implicit limit state functions from the viewpoint of statistical learning theory},
	Volume = {26},
	Year = {2004}}

@article{Hurtado-Alvarez2003,
	Author = {J.E. Hurtado and D.A. Alvarez},
	Journal = {Journal of Structural Engineering},
	Number = {8},
	Pages = {1141-1149},
	Title = {Classification approach for reliability analysis with stochastic finite-element modeling},
	Volume = {129},
	Year = {2003}}

@article{Ishibuchi90,
	Author = {H. Ishibuchi and H. Tanaka and N. Fukuoka},
	Journal = {International Journal of General Systems},
	Number = {4},
	Owner = {LVU},
	Pages = {311-329},
	Timestamp = {2010.12.11},
	Title = {Discriminant analysis of multi-dimensional interval data and its application to chemical sensing},
	Volume = {16},
	Year = {1990}}

@Article{Jeyakumar-12,
  author    = {V. Jeyakumar and G. Li and S. Suthaharan},
  journal   = {Optimization},
  title     = {Support vector machine classifiers with uncertain knowledge sets via robust optimization},
  year      = {2012},
  number    = {0},
  pages     = {1-18},
  volume    = {0},
  abstract  = {In this article we study support vector machine (SVM) classifiers
	in the face of uncertain knowledge sets and show how data uncertainty
	in knowledge sets can be treated in SVM classification by employing
	robust optimization. We present knowledge-based SVM classifiers with
	uncertain knowledge sets using convex quadratic optimization duality.
	We show that the knowledge-based SVM, where prior knowledge is in
	the form of uncertain linear constraints, results in an uncertain
	convex optimization problem with a set containment constraint. Using
	a new extension of Farkas' lemma, we reformulate the robust counterpart
	of the uncertain convex optimization problem in the case of interval
	uncertainty as a convex quadratic optimization problem. We then reformulate
	the resulting convex optimization problems as a simple quadratic
	optimization problem with non-negativity constraints using the Lagrange
	duality. We obtain the solution of the converted problem by a fixed
	point iterative algorithm and establish the convergence of the algorithm.
	We finally present some preliminary results of our computational
	experiments of the method.},
  owner     = {LVU},
  timestamp = {2012.10.27},
}

@book{Joachims-2002,
	Address = {Norwell, MA, USA},
	Author = {T. Joachims},
	Owner = {LVU},
	Publisher = {Kluwer Academic Publishers},
	Timestamp = {2012.10.27},
	Title = {Learning to Classify Text Using Support Vector Machines: Methods, Theory and Algorithms},
	Year = {2002}}

@article{Kotsiantis2006,
	Author = {S. Kotsiantis and D. Kanellopoulos and P. Pintelas},
	Journal = {GESTS International Transactions on Computer Science and Engineering},
	Number = {1},
	Pages = {25-36},
	Title = {Handling imbalanced datasets: A review},
	Volume = {30},
	Year = {2006}}

@incollection{Kunapuli-2010,
	Author = {G. Kunapuli and K. Bennett and A. Shabbeer and R. Maclin and J. Shavlik},
	Booktitle = {Machine Learning and Knowledge Discovery in Databases},
	Owner = {LVU},
	Pages = {145-161},
	Publisher = {Springer Berlin / Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Timestamp = {2012.10.28},
	Title = {Online Knowledge-Based Support Vector Machines},
	Volume = {6322},
	Year = {2010}}

@book{Kuncheva-2004,
	Address = {New Jersey},
	Author = {L.I. Kuncheva},
	Publisher = {Wiley-Interscience},
	Title = {Combining Pattern Classifiers: Methods and Algorithms},
	Year = {2004}}

@article{Hosseinzadeh07,
	Author = {L.F.Hosseinzadeh and G.R. Jahanshahloo and F.R. Balf and H.Z. Rezai},
	Journal = {Applied Mathematical Sciences},
	Number = {15},
	Owner = {LVU},
	Pages = {723-737},
	Timestamp = {2010.12.11},
	Title = {Discriminate analysis of imprecise data},
	Volume = {1},
	Year = {2007}}

@conference{Lahdelma2010420,
	Author = {R. Lahdelma and P. Salminen},
	Document_Type = {Conference Paper},
	Journal = {Proceedings of the 10th IASTED International Conference on Artificial Intelligence and Applications, AIA 2010},
	Owner = {LVU},
	Pages = {420-425},
	Timestamp = {2010.11.15},
	Title = {A method for ordinal classification in multicriteria decision making},
	Year = {2010}}

@article{Lanckriet-2002,
	Author = {G.R.G. Lanckriet and L.E. Ghaoui and C. Bhattacharyya and M.I. Jordan},
	Journal = {Journal of Machine Learning Research},
	Owner = {LVU},
	Pages = {555-582},
	Timestamp = {2012.02.20},
	Title = {A robust minimax approach to classification},
	Volume = {3},
	Year = {2002}}

@incollection{Lanckriet-2003,
	Address = {Cambridge, MA},
	Author = {G.R.G. Lanckriet and L.E. Ghaoui and M.I. Jordan},
	Booktitle = {Advances in Neural Information Processing Systems},
	Editor = {S. Becker and S. Thrun and K. Obermayer},
	Journal = {Journal of Machine Learning Research},
	Pages = {905-912},
	Publisher = {MIT Press},
	Title = {Robust novelty detection with single-class MPM},
	Volume = {15},
	Year = {2003}}

@article{Lauer-Bloch-08,
	Author = {F. Lauer and G. Bloch},
	Journal = {Neurocomputing},
	Number = {7-9},
	Owner = {LVU},
	Pages = {1578-1594},
	Timestamp = {2012.10.27},
	Title = {Incorporating prior knowledge in support vector machines for classification: {A} review},
	Volume = {71},
	Year = {2008}}

@article{Lauer-Bloch-08a,
	Abstract = {This paper explores the addition of constraints to the linear programming
	formulation of the support vector regression problem for the incorporation
	of prior knowledge. Equality and inequality constraints are studied
	with the corresponding types of prior knowledge that can be considered
	for the method. These include particular points with known values,
	prior knowledge on any derivative of the function either provided
	by a prior model or available only at some specific points and bounds
	on the function or any derivative in a given domain. Moreover, a
	new method for the simultaneous approximation of multiple outputs
	linked by some prior knowledge is proposed. This method also allows
	consideration of different types of prior knowledge on single outputs
	while training on multiple outputs. Synthetic examples show that
	incorporating a wide variety of prior knowledge becomes easy, as
	it leads to linear programs, and helps to improve the approximation
	in difficult cases. The benefits of the method are finally shown
	on a real-life application, the estimation of in-cylinder residual
	gas fraction in spark ignition engines, which is representative of
	numerous situations met in engineering.},
	Author = {F. Lauer and G. Bloch},
	Journal = {Machine Learning},
	Number = {1},
	Owner = {LVU},
	Pages = {89-118},
	Timestamp = {2012.10.27},
	Title = {Incorporating prior knowledge in support vector regression},
	Volume = {70},
	Year = {2008}}

@article{Lee-Mangasarian-Wolberg-2003,
	Author = {Lee, Y.-J. and Mangasarian, O.L. and Wolberg, W.H.},
	Journal = {Computational Optimization and Applications},
	Number = {1-3},
	Pages = {151-166},
	Title = {Survival-time classification of breast cancer patients},
	Volume = {25},
	Year = {2003}}

@article{Li-Liu-2012,
	Address = {Los Alamitos, CA, USA},
	Author = {Der-Chiang Li and Chiao-Wen Liu},
	Journal = {IEEE Transactions on Knowledge and Data Engineering},
	Number = {3},
	Pages = {452-464},
	Publisher = {IEEE Computer Society},
	Title = {Extending attribute information for small data set classification},
	Volume = {24},
	Year = {2012}}

@article{Li-Jeyakumar-Lee-11,
	Abstract = {In this paper we present a robust conjugate duality theory for convex
	programming problems in the face of data uncertainty within the framework
	of robust optimization, extending the powerful conjugate duality
	technique. We first establish robust strong duality between an uncertain
	primal parameterized convex programming model problem and its uncertain
	conjugate dual by proving strong duality between the deterministic
	robust counterpart of the primal model and the optimistic counterpart
	of its dual problem under a regularity condition. This regularity
	condition is not only sufficient for robust duality but also necessary
	for it whenever robust duality holds for every linear perturbation
	of the objective function of the primal model problem. More importantly,
	we show that robust strong duality always holds for partially finite
	convex programming problems under scenario data uncertainty and that
	the optimistic counterpart of the dual is a tractable finite dimensional
	problem. As an application, we also derive a robust conjugate duality
	theorem for support vector machines which are a class of important
	convex optimization models for classifying two labelled data sets.
	The support vector machine has emerged as a powerful modelling tool
	for machine learning problems of data classification that arise in
	many areas of application in information and computer sciences.},
	Author = {G.Y. Li and V. Jeyakumar and G.M. Lee},
	Journal = {Nonlinear Analysis: Theory, Methods \& Applications},
	Number = {6},
	Pages = {2327-2341},
	Title = {Robust conjugate duality for convex optimization under uncertainty with application to data classification},
	Volume = {74},
	Year = {2011}}

@article{Li-Ridder-Duin-Reinders-08,
	Author = {Yunlei Li and D. de Ridder and R.P.W. Duin and M.J.T. Reinders},
	Journal = {Pattern Recognition},
	Owner = {LVU},
	Pages = {320-330},
	Timestamp = {2012.10.27},
	Title = {Integration of prior knowledge of measurement noise in kernel density classification},
	Volume = {41},
	Year = {2008}}

@misc{Lichman:2013,
	Author = {M. Lichman},
	Institution = {University of California, Irvine, School of Information and Computer Sciences},
	Title = {{UCI} Machine Learning Repository},
	Url = {http://archive.ics.uci.edu/ml},
	Year = {2013},
	Bdsk-Url-1 = {http://archive.ics.uci.edu/ml}}

@article{Loo-2014,
	Author = {M.P.J. van der Loo (2014). The stringdist package for approximate string matching. R Journal 6(1) pp. 111-122},
	Journal = {R Journal},
	Number = {1},
	Owner = {lev u},
	Pages = {111-122},
	Timestamp = {2015.10.11},
	Title = {The stringdist package for approximate string matching},
	Volume = {6},
	Year = {2014}}

@article{Lu-Wang-Utiyama-09,
	Author = {Lu, Baoliang and Wang, Xiaolin and Utiyama, Masao},
	Journal = {Frontiers of Computer Science in China},
	Number = {1},
	Owner = {LVU},
	Pages = {109-122},
	Timestamp = {2012.10.28},
	Title = {Incorporating prior knowledge into learning by dividing training data},
	Volume = {3},
	Year = {2009}}

@article{Mangasarian-2005,
	Author = {O.L. Mangasarian},
	Journal = {SIAM J. on Optimization},
	Number = {2},
	Numpages = {8},
	Pages = {375-382},
	Title = {Knowledge-Based Linear Programming},
	Volume = {15},
	Year = {2005}}

@article{Masson-Denoeux2004,
	Author = {M.-H. Masson and T. Denoeux},
	Journal = {Pattern Recognition Letters},
	Number = {2},
	Owner = {LVU},
	Pages = {163-171},
	Timestamp = {2011.01.28},
	Title = {Clustering interval-valued data using belief functions},
	Volume = {25},
	Year = {2004}}

@inproceedings{Nivlet-Fournier-Royer01,
	Address = {Ithaca, NY, USA},
	Author = {P. Nivlet and F. Fournier and J.-J. Royer},
	Booktitle = {Second International Symposium on Imprecise Probabilities and Their Applications},
	Owner = {LVU},
	Pages = {284-292},
	Timestamp = {2010.12.11},
	Title = {Interval discriminant analysis: An efficient method to integrate errors in supervised pattern recognition},
	Year = {2001}}

@article{Notterman-2001,
	Author = {D.A. Notterman and U. Alon and A.J. Sierk and A.J. Levine},
	Journal = {Cancer Research},
	Month = {April},
	Owner = {lvu},
	Pages = {3124-3130},
	Timestamp = {2012.08.03},
	Title = {Transcriptional gene expression profiles of colorectal adenoma, adenocarcinoma, and normal tissue examined by oligonucleotide arrays},
	Volume = {61},
	Year = {2001}}

@inproceedings{Pavlidis-01,
	Address = {New York, NY, USA},
	Author = {Pavlidis, Paul and Weston, Jason and Cai, Jinsong and Grundy, William Noble},
	Booktitle = {Proceedings of the fifth annual international conference on Computational biology},
	Owner = {LVU},
	Pages = {249-255},
	Publisher = {ACM},
	Timestamp = {2012.10.28},
	Title = {Gene functional classification from heterogeneous data},
	Year = {2001}}

@article{Provost-Fawcett-2001,
	Author = {F. Provost and T. Fawcett},
	Journal = {Machine Learning},
	Number = {3},
	Pages = {203-231},
	Title = {Robust classification for imprecise environments},
	Volume = {42},
	Year = {2001}}

@manual{Ridgeway-2005,
	Author = {G. Ridgeway},
	Citeseerurl = {http://cran.r-project.org/doc/packages/gbm.pdf.},
	Owner = {LVU},
	Timestamp = {2012.09.29},
	Title = {{GBM} 1.5 package manual},
	Year = {2005},
	Bdsk-Url-1 = {http://cran.r-project.org/doc/packages/gbm.pdf.}}

@article{Rocco-Moreno2002,
	Abstract = {This paper deals with the feasibility of using support vector machine
	(SVM) to build empirical models for use in reliability evaluation.
	The approach takes advantage of the speed of SVM in the numerous
	model calculations typically required to perform a Monte Carlo reliability
	evaluation. The main idea is to develop an estimation algorithm,
	by training a model on a restricted data set, and replace system
	performance evaluation by a simpler calculation, which provides reasonably
	accurate model outputs. The proposed approach is illustrated by several
	examples. Excellent system reliability results are obtained by training
	a SVM with a small amount of information.},
	Author = {C.M. Rocco and J.A. Moreno},
	Journal = {Reliability Engineering \& System Safety},
	Number = {3},
	Pages = {237 - 243},
	Title = {Fast Monte Carlo reliability evaluation using support vector machine},
	Volume = {76},
	Year = {2002}}

@incollection{Sanseverino-Moreno2002,
	Abstract = {In this paper we propose the use of Support Vector Machine (SVM) to
	evaluate system reliability. The main idea is to develop an estimation
	algorithm by training a SVM on a restricted data set, replacing the
	system performance model evaluation by a simpler calculation. The
	proposed approach is illustrated by an example. System reliability
	is properly emulated by training a SVM with a small amount of information.},
	Author = {C. Sanseverino and J. Moreno},
	Booktitle = {Computational Science (ICCS 2002)},
	Editor = {P. Sloot and A. Hoekstra and C. Tan and J. Dongarra},
	Pages = {147-155},
	Publisher = {Springer Berlin / Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Reliability evaluation using {M}onte {C}arlo simulation and support vector machine},
	Volume = {2329},
	Year = {2002}}

@article{Schaible-Shi-2003,
	Author = {S. Schaible and J. Shi},
	Journal = {Optimization Methods and Software},
	Pages = {219--229},
	Title = {Fractional programming: the sum-of-ratios case},
	Volume = {18},
	Year = {2003}}

@inproceedings{Scholkopf-Simard-98,
	Address = {Cambridge},
	Author = {B. Scholkopf and P. Simard and A. Smola and V. Vapnik},
	Booktitle = {Advances in neural information processing systems. Proceedings of the 1997 conference.},
	Owner = {LVU},
	Pages = {640-646},
	Publisher = {MIT Press},
	Timestamp = {2012.10.27},
	Title = {Prior knowledge in support vector kernels},
	Volume = {10},
	Year = {1998}}

@book{Scholkopf-Smola02,
	Address = {Cambridge, Massachusetts},
	Author = {B. Scholkopf and A.J. Smola},
	Publisher = {The MIT Press},
	Title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
	Year = {2002}}

@article{Shao1989,
	Author = {J. Shao},
	Journal = {Journal of the American Statistical Association},
	Owner = {LVU},
	Pages = {727-732},
	Timestamp = {2011.05.09},
	Title = {Monte {C}arlo approximations in {B}ayesian decision theory},
	Volume = {84},
	Year = {1989}}

@article{Silva-Brito06,
	Author = {A. Silva and P. Brito},
	Issue = {2},
	Journal = {Computational Statistics},
	Pages = {289-308},
	Publisher = {Physica Verlag},
	Title = {Linear discriminant analysis for interval data},
	Volume = {21},
	Year = {2006}}

@inproceedings{Small-2011,
	Address = {Bellevue, WA, USA},
	Author = {K. Small and B. Wallace and C. Brodley and T. Trikalinos},
	Booktitle = {Proc. of the 28th International Conference on Machine Learning (ICML).},
	Owner = {LVU},
	Pages = {865-872},
	Publisher = {Omnipress},
	Timestamp = {2012.10.27},
	Title = {The constrained weight space svm: learning with ranked features},
	Year = {2011}}

@article{Smola-Scholkopf-2004,
	Author = {A.J. Smola and B. Scholkopf},
	Issue = {3},
	Journal = {Statistics and Computing},
	Pages = {199-222},
	Title = {A tutorial on support vector regression},
	Volume = {14},
	Year = {2004}}

@article{Sollich2002,
	Abstract = {I describe a framework for interpreting Support Vector Machines (SVMs)
	as maximum a posteriori (MAP) solutions to inference problems with
	Gaussian Process priors. This probabilistic interpretation can provide
	intuitive guidelines for choosing a `good' SVM kernel. Beyond this,
	it allows Bayesian methods to be used for tackling two of the outstanding
	challenges in SVM classification: how to tune hyperparameters---the
	misclassification penalty C, and any parameters specifying the ernel---and
	how to obtain predictive class probabilities rather than the conventional
	deterministic class label predictions. Hyperparameters can be set
	by maximizing the evidence; I explain how the latter can be defined
	and properly normalized. Both analytical approximations and numerical
	methods (Monte Carlo chaining) for estimating the evidence are discussed.
	I also compare different methods of estimating class probabilities,
	ranging from simple evaluation at the MAP or at the posterior average
	to full averaging over the posterior. A simple toy application illustrates
	the various concepts and techniques.},
	Author = {P. Sollich},
	Issue = {1},
	Journal = {Machine Learning},
	Pages = {21-52},
	Title = {Bayesian methods for {S}upport {V}ector {M}achines: {E}vidence and predictive class probabilities},
	Volume = {46},
	Year = {2002}}

@book{Steinwart-Christmann-2008,
	Address = {New York},
	Author = {I. Steinwart and A. Christmann},
	Owner = {lev u},
	Publisher = {Springer-Verlag},
	Timestamp = {2015.09.26},
	Title = {Support Vector Machines},
	Year = {2008}}

@article{Sun-Wang-Lim-DeJong-07,
	Author = {Sun, Qiang and Wang, Li-Lun and Lim, ShiauHong and DeJong, Gerald},
	Doi = {10.1007/s10032-007-0053-1},
	Issn = {1433-2833},
	Issue = {3-4},
	Journal = {International Journal of Document Analysis and Recognition},
	Keywords = {Explanation-based learning; Support vector machine; Domain knowledge; Character recognition; Machine learning},
	Language = {English},
	Number = {3-4},
	Owner = {LVU},
	Pages = {175-186},
	Publisher = {Springer-Verlag},
	Timestamp = {2012.10.28},
	Title = {Robustness through prior knowledge: using explanation-based learning to distinguish handwritten {C}hinese characters},
	Url = {http://dx.doi.org/10.1007/s10032-007-0053-1},
	Volume = {10},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10032-007-0053-1}}

@article{Sun-Zhang-Wang-08,
	Abstract = {In some sample based regression tasks, the observed samples are quite
	few or not informative enough. As a result, the conflict between
	the number of samples and the model complexity emerges, and the regression
	method will confront the dilemma whether to choose a complex model
	or not. Incorporating the prior knowledge is a potential solution
	for this dilemma. In this paper, a sort of the prior knowledge is
	investigated and a novel method to incorporate it into the kernel
	based regression scheme is proposed. The proposed prior knowledge
	based kernel regression (PKBKR) method includes two subproblems:
	representing the prior knowledge in the function space, and combining
	this representation and the training samples to obtain the regression
	function. A greedy algorithm for the representing step and a weighted
	loss function for the incorporation step are proposed. Finally, experiments
	are performed to validate the proposed PKBKR method, wherein the
	results show that the proposed method can achieve relatively high
	regression performance with appropriate model complexity, especially
	when the number of samples is small or the observation noise is large.},
	Author = {Z. Sun and Z.-K. Zhang and H.-G. Wang},
	Journal = {Acta Automatica Sinica},
	Number = {12},
	Pages = {1515 - 1521},
	Title = {Incorporating prior knowledge into kernel based regression},
	Volume = {34},
	Year = {2008}}

@article{Suykens-Vandewalle-1999,
	Author = {J.A.K. Suykens and J. Vandewalle},
	Journal = {Neural Processing Letters},
	Owner = {LVU},
	Pages = {293-300},
	Timestamp = {2010.12.24},
	Title = {Least squares support vector machine classifiers},
	Volume = {9},
	Year = {1999}}

@article{Tai-Pan-07,
	Author = {Feng Tai and Wei Pan},
	Journal = {Bioinformatics},
	Number = {14},
	Owner = {LVU},
	Pages = {1775-1782},
	Timestamp = {2012.10.27},
	Title = {Incorporating prior knowledge of predictors into penalized classifiers with multiple penalty terms},
	Volume = {23},
	Year = {2007}}

@article{Takahashi-Kudo-Nakamura-2011,
	Abstract = {We propose an algorithm to approximate each class region by a small
	number of approximated convex hulls and to use these for classification.
	The classifier is one of non-kernel maximum margin classifiers. It
	keeps the maximum margin in the original feature space, unlike support
	vector machines with a kernel. The construction of an exact convex
	hull requires an exponential time in dimension, so we find an approximate
	convex hull (a polyhedron) instead, which is constructed in linear
	time in dimension. We also propose a model selection procedure to
	control the number of faces of convex hulls for avoiding over-fitting,
	in which a fast procedure is adopted to calculate an upper-bound
	of the leave-one-out error. In comparison with support vector machines,
	the proposed approach is shown to be comparable in performance but
	more natural in the extension to multi-class problems.},
	Author = {T. Takahashi and M. Kudo and A. Nakamura},
	Journal = {Pattern Recognition Letters},
	Pages = {-},
	Title = {Construction of convex hull classifiers in high dimensions},
	Volume = {In Press, Corrected Proof},
	Year = {2011}}

@article{Tarigan-Geer2008,
	Author = {B. Tarigan and S.A. van de Geer},
	Journal = {Journal of Machine Learning Research},
	Owner = {LVU},
	Pages = {2171-2185},
	Timestamp = {2011.02.08},
	Title = {A moment bound for multi-hinge classifiers},
	Volume = {9},
	Year = {2008}}

@book{Tikhonov-Arsenin-1977,
	Address = {W.H. Winston, Washington DC},
	Author = {A.N. Tikhonov and V.Y. Arsenin},
	Owner = {LVU},
	Timestamp = {2010.12.13},
	Title = {Solution of Ill-Posed Problems},
	Year = {1977}}

@article{Tipping-2001,
	Author = {M. Tipping},
	Journal = {Journal of Machine Learning Research},
	Owner = {LVU},
	Pages = {211-244},
	Timestamp = {2014.01.09},
	Title = {Sparse {B}ayesian learning and the relevance vector machine},
	Volume = {1},
	Year = {2001}}

@inproceedings{Tong+Koller:AAAI00,
	Address = {Austin, Texas},
	Author = {S. Tong and D. Koller},
	Booktitle = {Proceedings of the 17th National Conference on Artificial Intelligence (AAAI)},
	Month = {August},
	Pages = {658--664},
	Title = {Restricted {Bayes} optimal classifiers},
	Year = {2000}}

@article{Trafalis-Gilbert-2007,
	Author = {T.B. Trafalis and R.C. Gilbert},
	Journal = {Optimization Methods and Software},
	Number = {1},
	Pages = {187-198},
	Title = {Robust support vector machines for classification and computational issues},
	Volume = {22},
	Year = {2007}}

@inproceedings{Veillard-2011,
	Abstract = {SVMs with the general purpose RBF kernel are widely considered as
	state-of-the-art supervised learning algorithms due to their effectiveness
	and versatility. However, in practice, SVMs often require more training
	data than readily available. Prior-knowledge may be available to
	compensate this shortcoming provided such knowledge can be effectively
	passed on to SVMs. In this paper, we propose a method for the incorporation
	of prior-knowledge via an adaptation of the standard RBF kernel.
	Our practical and computationally simple approach allows prior-knowledge
	in a variety of forms ranging from regions of the input space as
	crisp or fuzzy sets to pseudo-periodicity. We show that this method
	is effective and that the amount of required training data can be
	largely decreased, opening the way for new usages of SVMs. We propose
	a validation of our approach for pattern recognition and classification
	tasks with publicly available datasets in different application domains.},
	Address = {Washington, DC, USA},
	Author = {A. Veillard and D. Racoceanu and S. Bressan},
	Booktitle = {Proceedings of the IEEE 23rd International Conference on Tools with Artificial Intelligence},
	Pages = {591-596},
	Publisher = {IEEE Computer Society},
	Title = {Incorporating prior-knowledge in support vector machines by kernel adaptation},
	Year = {2011}}

@article{Wang-Lu-Plataniotis-Lu-2009,
	Author = {J. Wang and H. Lu and K.N. Plataniotis and J. Lu},
	Journal = {Pattern Recognition},
	Number = {7},
	Pages = {1237 - 1247},
	Title = {Gaussian kernel optimization for pattern classification},
	Volume = {42},
	Year = {2009}}

@article{Wang-Zhao-Hoi-13,
	Abstract = {Both "cost-sensitive classification" and "online learning" have been
	extensively studied in data mining and machine learning communities,
	respectively. There was however very limited study for addressing
	an important intersecting problem, that is, "Cost-Sensitive Online
	Classification". In this paper, we formally study this problem, and
	propose a new framework for Cost-Sensitive Online Classification
	by exploiting the idea of online gradient descent techniques. Based
	on the framework, we propose two cost-sensitive online classification
	algorithms, which are designed to directly optimize two well-known
	cost-sensitive measures: (i) maximization of weighted sum of "sensitivity"
	and "specificity", and (ii) minimization of weighted "misclassification
	cost". We analyze the theoretical bounds of the cost-sensitive measures
	made by the proposed algorithms, and extensively examine their empirical
	performance on a variety of cost-sensitive online classification
	tasks. Finally, we demonstrate the application of the proposed technique
	for solving several online anomaly detection tasks, showing that
	the proposed technique could be a highly efficient and effective
	tool to tackle cost-sensitive online classification tasks in various
	application domains.},
	Author = {Wang, J. and Zhao, P. and Hoi, S.},
	Doi = {10.1109/TKDE.2013.157},
	Journal = {IEEE Transactions on Knowledge and Data Engineering},
	Number = {99},
	Pages = {1-1},
	Title = {Cost-sensitive online classification},
	Volume = {PP},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/TKDE.2013.157}}

@inproceedings{Wang-Xue-Chan-04,
	Abstract = {SVM based image retrieval suffers from the scarcity of labelled samples.
	In this paper, this problem is solved by incorporating prior knowledge
	into SVM. Firstly, some prior knowledge of image retrieval is discussed
	and constructed. After that, the knowledge is incorporated into SVM
	optimization as a constraint, and a new knowledge-based target function
	is formulated. Based on this, a framework of image retrieval with
	knowledge based SVM is proposed. Experimental results demonstrate
	that the proposed method can effectively improve the learning and
	retrieval performance of SVM, especially when the number of labelled
	samples is small.},
	Address = {Los Alamitos, CA, USA},
	Author = {Lei Wang and Ping Xue and Kap Luk Chan},
	Booktitle = {Proceedings of the 17th International Conference on Pattern Recognition (ICPR'04)},
	Owner = {LVU},
	Pages = {981-984},
	Publisher = {IEEE Computer Society},
	Timestamp = {2012.10.27},
	Title = {Incorporating prior knowledge into {SVM} for image retrieval},
	Volume = {2},
	Year = {2004}}

@article{Wang-Xu-Lu-Zhang-2003,
	Author = {W. Wang and Z. Xu and W. Lu and X. Zhang},
	Journal = {Neurocomputing},
	Number = {3},
	Owner = {LVU},
	Pages = {643-663},
	Timestamp = {2015.01.14},
	Title = {Determination of the spread parameter in the {G}aussian kernel for classification and regression},
	Volume = {55},
	Year = {2003}}

@book{Webb2002,
	Author = {A.R. Webb},
	Owner = {LVU},
	Publisher = {Wiley},
	Timestamp = {2011.04.12},
	Title = {Statistical Pattern Recognition, 2nd Edition},
	Year = {2002}}

@article{Weiss:2004,
	Author = {G.M. Weiss},
	Issue = {1},
	Journal = {ACM SIGKDD Explorations Newsletter},
	Pages = {7--19},
	Publisher = {ACM},
	Title = {Mining with rarity: a unifying framework},
	Volume = {6},
	Year = {2004}}

@techreport{Weston-Watkins-98,
	Address = {Department of Computer Science},
	Author = {J. Weston and C. Watkins},
	Institution = {Royal Holloway, University of London},
	Month = {May},
	Number = {CSD-TR-98-04},
	Owner = {LVU},
	Timestamp = {2013.03.14},
	Title = {Multi-class support vector machines},
	Type = {Technical Report},
	Year = {1998}}

@techreport{Weston-Watkins1998,
	Address = {Egham, TW20 0EX, UK},
	Author = {J. Weston and C. Watkins},
	Institution = {Department of Computer Science, Royal Holloway, University of London},
	Number = {Technical Report CSD-TR-98-04},
	Owner = {LVU},
	Timestamp = {2011.04.14},
	Title = {Multi-class support vector machines},
	Year = {1998}}

@article{Xindong_Wu-2008,
	Author = {X. Wu and V. Kumar and Q.J. Ross and J. Ghosh and Q. Yang and H. Motoda and G.J. McLachlan and A. Ng and B. Liu and P. Yu and Z.-H. Zhou and M. Steinbach and D.J. Hand and D. Steinberg},
	Journal = {Knowledge and Information Systems},
	Number = {1},
	Owner = {LVU},
	Pages = {1-37},
	Timestamp = {2012.10.30},
	Title = {Top 10 algorithms in data mining},
	Volume = {14},
	Year = {2008}}

@inproceedings{Wu-Srihari-04,
	Address = {New York, NY, USA},
	Author = {Xiaoyun Wu and Rohini Srihari},
	Booktitle = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
	Pages = {326-333},
	Publisher = {ACM},
	Title = {Incorporating prior knowledge with weighted margin support vector machines},
	Year = {2004}}

@article{Xiang-Hu-Zhou-2011,
	Abstract = {Support vector machines for regression are implemented based on regularization
	schemes in reproducing kernel Hilbert spaces associated with an [epsilon]-insensitive
	loss. The insensitive parameter [epsilon]>0 changes with the sample
	size and plays a crucial role in the learning algorithm. The purpose
	of this paper is to present a perturbation theorem to show how the
	medium function of the probability measure for regression (with [epsilon]=0)
	can be approximated by learning the minimizer of the generalization
	error with sufficiently small parameter [epsilon]>0. A concrete learning
	rate is provided under a regularity condition of the medium function
	and a noise condition of the probability measure.},
	Author = {D.-H. Xiang and T. Hu and D.-X. Zhou},
	Journal = {Applied Mathematics Letters},
	Number = {12},
	Pages = {2107 - 2109},
	Title = {Learning with varying insensitive loss},
	Volume = {24},
	Year = {2011}}

@inproceedings{Xing-Pei-Yu-Wang-11,
	Author = {Z. Xing and J. Pei and P.S. Yu. and K. Wang},
	Booktitle = {Proceedings of the Eleventh SIAM International Conference on Data Mining},
	Owner = {LVU},
	Pages = {247-258},
	Publisher = {Omnipress},
	Timestamp = {2012.10.28},
	Title = {Extracting interpretable features for early classification on time series},
	Year = {2011}}

@article{Xu:2009,
	Author = {H. Xu and C. Caramanis and S. Mannor},
	Journal = {The Journal of Machine Learning Research},
	Number = {7},
	Pages = {1485--1510},
	Title = {Robustness and Regularization of Support Vector Machines},
	Volume = {10},
	Year = {2009}}

@inproceedings{Xu-Crammer-Schuurmans-2006,
	Address = {Boston, Massachusetts},
	Author = {L. Xu and K. Crammer and D. Schuurmans},
	Booktitle = {Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06)},
	Owner = {LVU},
	Pages = {536-542},
	Publisher = {AAAI Press; MIT Press},
	Timestamp = {2012.02.21},
	Title = {Robust support vector machine training via convex outlier ablation},
	Volume = {21},
	Year = {2006}}

@article{Yang-Song-Wang-07,
	Author = {X. Yang and Q. Song and Y. Wang},
	Journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	Number = {5},
	Owner = {LVU},
	Pages = {961-976},
	Timestamp = {2012.05.10},
	Title = {A weighted support vector machine for data classification},
	Volume = {21},
	Year = {2007}}

@article{Yuan-Wegkamp-2010,
	Author = {M. Yuan and M. Wegkamp},
	Journal = {Journal of Machine Learning Research},
	Owner = {LVU},
	Pages = {111-130},
	Timestamp = {2010.12.14},
	Title = {Classification methods with reject option based on convex risk minimization},
	Volume = {11},
	Year = {2010}}

@inproceedings{Zadrozny-Langford-Abe-03,
	Address = {Melbourne, FL},
	Author = {B. Zadrozny and J. Langford and N. Abe},
	Booktitle = {Proceedings of the Third IEEE International Conference on Data Mining},
	Owner = {lvu},
	Pages = {435-442},
	Timestamp = {2012.07.26},
	Title = {Cost-sensitive learning by cost proportionale example weighting},
	Year = {2003}}

@article{Zaffalon-2005,
	Abstract = {Classifiers that aim at doing credible predictions should rely on
	carefully elicited prior knowledge. Often this is not available so
	they should start learning from data in condition of near-ignorance.
	This paper shows empirically, on an agricultural data set, that established
	methods of classification do not always adhere to this principle.
	Traditional ways to represent prior ignorance are shown to have an
	overwhelming weight compared to the information in the data, producing
	overconfident predictions. This point is crucial for problems, such
	as environmental ones, where prior knowledge is often scarce and
	even the data may not be known precisely. Credal classification,
	and in particular the naive credal classifier, is proposed as more
	faithful ways to cope with the ignorance problem. With credal classification,
	conditions of ignorance may limit the power of the inferences, not
	the credibility of the predictions.},
	Author = {M. Zaffalon},
	Journal = {Environmental Modelling and Software},
	Number = {8},
	Pages = {1003 - 1012},
	Title = {Credible classification for environmental problems},
	Volume = {20},
	Year = {2005}}

@article{Zhang-Yang-Qian-2012,
	Author = {N. Zhang and J. Yang and J.-j. Qian},
	Journal = {Pattern Recognition Letters},
	Number = {13},
	Pages = {1689-1694},
	Title = {Component-based global k-NN classifier for small sample size problems},
	Volume = {33},
	Year = {2012}}

@article{Zhang04,
	Author = {Tong Zhang},
	Journal = {The Annals of Statistics},
	Pages = {1-28},
	Title = {Statistical behavior and consistency of classification methods based on convex risk minimization},
	Year = {2004}}

@article{Zhao-Zhong-Zhao-2011,
	Abstract = {As a kernel-based method, whether the selected kernel matches the
	data determines the performance of support vector machine. Conventional
	support vector classifiers are not suitable to the imbalanced learning
	tasks since they tend to classify the instances to the majority class
	which is the less important class. In this paper, we propose a weighted
	maximum margin criterion to optimize the data-dependent kernel, which
	makes the minority class more clustered in the induced feature space.
	We train support vector classification with the optimal kernel. The
	experimental results on nine benchmark data sets indicate the effectiveness
	of the proposed algorithm for imbalanced data classification problems.},
	Author = {Z. Zhao and P. Zhong and Y. Zhao},
	Journal = {Mathematical and Computer Modelling},
	Number = {3-4},
	Pages = {1093 - 1099},
	Title = {Learning SVM with weighted maximum margin criterion for classification of imbalanced data},
	Volume = {54},
	Year = {2011}}

@InBook{Awad-Khanna-2015,
  author    = {M. Awad and R. Khanna},
  chapter   = {Support Vector Regression},
  pages     = {67--80},
  publisher = {Apress},
  title     = {Efficient Learning Machines: Theories, Concepts, and Applications for Engineers and System Designers},
  year      = {2015},
  address   = {Berkeley, CA},
}

@Article{Brier-1950,
  author  = {G.W. Brier},
  journal = {Monthly Weather Review},
  title   = {Verification of forecasts expressed in terms of probability},
  year    = {1950},
  number  = {1},
  pages   = {1--3},
  volume  = {78},
}

@InProceedings{Duan-etal-09,
  author    = {Y. Duan and R.G. Harley and T.G. Habetler},
  booktitle = {The 6th International Power Electronics and Motion Control Conference},
  title     = {Comparison of Particle Swarm Optimization and Genetic Algorithm in the Design of Permanent Magnet Motors},
  year      = {2009},
  pages     = {822-825},
  publisher = {IEEE},
}

@Article{Sooda-Nair-11,
  author  = {K. Sooda and T.R.G. Nair},
  journal = {International Journal of Computer Applications},
  title   = {A Comparative Analysis for Determining the Optimal Path using {PSO} and {GA}},
  year    = {2011},
  number  = {4},
  pages   = {8--12},
  volume  = {32},
}

@Article{Wang-Zheng-Xu-08,
  author  = {S. Wang and F. Zheng and L. Xu},
  journal = {Journal of Advanced Manufacturing Systems},
  title   = {Comparison between particle swarm optimization and genetic algorithm in artificial neural network for life prediction of nc tools},
  year    = {2008},
  number  = {1},
  pages   = {1--7},
  volume  = {7},
}

@Article{Panda-Padhy-08,
  author  = {S. Panda and N.P. Padhy},
  journal = {Applied Soft Computing},
  title   = {Comparison of particle swarm optimization and genetic algorithm for {FACTS}-based controller design},
  year    = {2008},
  number  = {4},
  pages   = {1418--1427},
  volume  = {8},
}

@Comment{jabref-meta: databaseType:bibtex;}
