% Encoding: windows-1251

@InProceedings{Hamdan-Hajjar-2011,
  author    = {H. Hamdan and C. Hajjar},
  title     = {A neural networks approach to interval-valued data clustering. Applicationto Lebanese meteorological stations data},
  booktitle = {2011 IEEE Workshop on Signal Processing Systems (SiPS)},
  year      = {2011},
  pages     = {373-378},
  month     = {October},
  publisher = {IEEE},
  abstract  = {The Self-Organizing Maps have been widely used as multidimensional unsupervised classifiers. The aim of this paper is to develop a self-organizing map for interval data. Due to the increasing use of such data in Data Mining, many clustering methods for interval data have been proposed this last decade. In this paper, we propose an algorithm to train the self-organizing map for interval data. We use the Euclidian distance to compare two vectors of intervals. In order to show the usefulness of our approach, we apply the self-organizing map on real interval data issued from meteorological stations in Lebanon.},
}

@Article{Ak-Vitelli-Zio-2015,
  author    = {R. Ak and V. Vitelli and E. Zio},
  title     = {An interval-valued neural network approach for uncertainty quantification in short-term wind speed prediction},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2015},
  volume    = {26},
  number    = {11},
  pages     = {2787-2800},
  month     = {Nov},
  owner     = {lev u},
  timestamp = {2017.02.07},
}

@InCollection{Rossi-Conan-Guez-2002,
  author    = {F. Rossi and B. Conan-Guez},
  title     = {Multi-layer perceptron on interval data},
  booktitle = {Classification, Clustering, and Data Analysis},
  publisher = {Springer},
  year      = {2002},
  pages     = {427-434},
  address   = {Berlin Heidelberg},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@Article{Lu-etal-2015a,
  author    = {J. Lu and S. Xue and X. Zhang and Y. Han},
  title     = {A Neural Network-Based Interval Pattern Matcher},
  journal   = {Information},
  year      = {2015},
  volume    = {6},
  pages     = {388-398},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@Article{Yang-Wu-2012,
  author    = {D. Yang and W. Wu},
  title     = {A Smoothing Interval Neural Network},
  journal   = {Discrete Dynamics in Nature and Society},
  year      = {2012},
  volume    = {Article ID 456919},
  pages     = {1-25},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@InProceedings{Cimino-etal-2011,
  author    = {M.G.C.A. Cimino and B. Lazzerini and F. Marcelloni and W. Pedrycz},
  title     = {Granular data regression with neural networks},
  booktitle = {International Workshop on Fuzzy Logic and Applications},
  year      = {2011},
  pages     = {172-179},
  address   = {Berlin Heidelberg},
  publisher = {Springer},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@Article{Chakraverty-Sahoo-2014,
  author    = {S. Chakraverty and D.M. Sahoo},
  title     = {Interval response data based system identification of multi storey shear buildings using interval neural network modelling},
  journal   = {Computer Assisted Methods in Engineering and Science},
  year      = {2014},
  volume    = {21},
  pages     = {123-140},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@Article{Maia-Carvalho-2011,
  author    = {A.L.S. Maia and F.D.A. de Carvalho},
  title     = {Holt’s exponential smoothing and neural network models for forecasting interval-valued time series},
  journal   = {International Journal of Forecasting},
  year      = {2011},
  volume    = {27},
  number    = {3},
  pages     = {740-759.},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@Article{Cimino-etal-2014,
  author    = {M.G. Cimino and B. Lazzerini and F. Marcelloni and W. Pedrycz},
  title     = {Genetic interval neural networks for granular data regression},
  journal   = {Information Sciences},
  year      = {2014},
  volume    = {257},
  pages     = {313-330},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@Article{Yang-Li-Wu-2016,
  author    = {D. Yang and Z. Li and W. Wu},
  title     = {Extreme learning machine for interval neural networks},
  journal   = {Neural Computing and Applications},
  year      = {2016},
  volume    = {27},
  number    = {1},
  pages     = {3--8},
  owner     = {lev u},
  timestamp = {2017.02.08},
}

@Article{Luis-etal-2008,
  author   = {A. Luis and S. Maia and F.D.A. de Carvalho and T.B. Ludermir},
  title    = {Forecasting models for interval-valued time series},
  journal  = {Neurocomputing},
  year     = {2008},
  volume   = {71},
  number   = {16–18},
  pages    = {3344-3352},
  abstract = {This paper presents approaches to interval-valued time series forecasting. The first and second approaches are based on the autoregressive (AR) and autoregressive integrated moving average (ARIMA) models, respectively. The third approach is based on an artificial neural network (ANN) model and the last is based on a hybrid methodology that combines both ARIMA and ANN models. Each approach fits, respectively, two models on the mid-point and range of the interval values assumed by the interval-valued time series in the learning set. The forecasting of the lower and upper bounds of the interval value of the time series is accomplished through a combination of forecasts from the mid-point and range of the interval values. The evaluation of the models presented is based on the estimation of the average behavior of the mean absolute error and mean squared error in the framework of a Monte Carlo experiment. The results demonstrate that the approaches are useful in forecasting alternatives for interval-valued time series and indicate that the hybrid model is an effective way to improve the forecasting accuracy achieved by any one of the models separately.},
}

@Article{Chetwynd-Worden-Manson-2006,
  author  = {D. Chetwynd and K. Worden and G. Manson},
  title   = {An application of interval-valued neural networks to a regression problem},
  journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  year    = {2006},
  volume  = {462},
  number  = {2074},
  pages   = {3097-3114},
}

@Article{Hamdan-Hajjar-2013,
  author    = {H. Hamdan and C. Hajjar},
  title     = {Interval data clustering using self-organizing maps based on adaptive Mahalanobis distances},
  journal   = {Neural Networks},
  year      = {2013},
  volume    = {46},
  pages     = {124-132},
  abstract  = {The Self-Organizing Maps have been widely used as multidimensional unsupervised classifiers. The aim of this paper is to develop a self-organizing map for interval data. Due to the increasing use of such data in Data Mining, many clustering methods for interval data have been proposed this last decade. In this paper, we propose an algorithm to train the self-organizing map for interval data. We use the Euclidian distance to compare two vectors of intervals. In order to show the usefulness of our approach, we apply the self-organizing map on real interval data issued from meteorological stations in Lebanon.},
  booktitle = {2011 IEEE Workshop on Signal Processing Systems (SiPS)},
  publisher = {IEEE},
}

@InCollection{Hamdan-Hajjar-2012,
  author    = {H. Hamdan and C. Hajjar},
  title     = {Self-organizing map based on city-block distance for interval-valued data},
  booktitle = {Complex Systems Design \& Management},
  publisher = {Springer},
  year      = {2012},
  chapter   = {20},
  pages     = {281-292},
  address   = {Berlin, Heidelberg},
}

@Article{Ding-etal-2014,
  author   = {S. Ding and H. Jia and J. Chen and F. Jin},
  title    = {Granular neural networks},
  journal  = {Artificial Intelligence Review},
  year     = {2014},
  volume   = {41},
  number   = {3},
  pages    = {373-384},
  abstract = {Fuzzy neural networks (FNNs) and rough neural networks (RNNs) both have been hot research topics in the artificial intelligence in recent years. The former imitates the human brain in dealing with problems, the other takes advantage of rough set theory to process questions uncertainly. The aim of FNNs and RNNs is to process the massive volume of uncertain information, which is widespread applied in our life. This article summarizes the recent research development of FNNs and RNNs (together called granular neural networks). First the fuzzy neuron and rough neuron is introduced; next FNNs are analysed in two categories: normal FNNs and fuzzy logic neural networks; then the RNNs are analysed in the following four aspects: neural networks based on using rough sets in preprocessing information, neural networks based on rough logic, neural networks based on rough neuron and neural networks based on rough-granular; then we give a flow chart of the RNNs processing questions and an application of classical neural networks based on rough sets; next this is compared with FNNs and RNNs and the way to integrate is described; finally some advice is given on development of FNNs and RNNs in future.},
}

@Article{Cao-Wang-Li-2016,
  author   = {S. Cao and Y. Wang and J. Li},
  title    = {Approximation of fuzzy neural networks based on Choquet integral},
  journal  = {Journal of Intelligent \& Fuzzy Systems},
  year     = {2016},
  volume   = {31},
  number   = {2},
  pages    = {691-698},
  abstract = {In this paper, we shall introduce a metric on the space of fuzzy-valued measurable functions by means of a kind of nonlinear integral — Choquet integral with respect to a capacity. We called it as Choquet integral norm. By using the metric between fuzzy-valued measurable functions, we discuss the mean approximation of regular fuzzy neural networks in the sense of Choquet integral. It is shown that any fuzzy-valued measurable function on R can be approximated in mean by four-layer regular fuzzy neural networks in the sense of Choquet integral norm.},
}

@Article{Adam-etal-2016,
  author  = {S.P. Adam and G.D. Magoulas and D.A. Karras and M.N. Vrahatis},
  title   = {Bounding the Search Space for Global Optimization of Neural Networks Learning Error: An Interval Analysis Approach},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  pages   = {1-40},
}

@Article{Schwenk-Bengio-2000,
  author  = {H. Schwenk and Y. Bengio},
  title   = {Boosting neural networks},
  journal = {Neural Computation},
  year    = {2000},
  volume  = {12},
  number  = {8},
  pages   = {1869--1887},
}

@InProceedings{Windeatt-Ghaderi-1999,
  author    = {T. Windeatt and R.Ghaderi},
  title     = {AdaBoost and neural networks},
  booktitle = {ESANN'1999 Proceedings - European Symposium on Artificial Neural Networks},
  year      = {1999},
  pages     = {123--128},
  address   = {Bruges, Belgium},
  month     = {April},
}

@Article{Zhou-Wu-Tang-2002,
  author  = {Z.-H. Zhou and J. Wu and W. Tang},
  title   = {Ensembling neural networks: many could be better than all},
  journal = {Artificial intelligence},
  year    = {2002},
  volume  = {137},
  number  = {1-2},
  pages   = {239--263},
}

@Book{CANN-1999,
  title     = {Combining Artificial Neural Nets: Ensemble and Modular Multi-Net Systems},
  publisher = {Springer-Verlag},
  year      = {1999},
  editor    = {A.J.C. Sharkey},
  address   = {New York},
}

@InProceedings{Haq-Tao-Yang-2011,
  author    = {Q.S. ul Haq and L. Tao and S. Yang},
  title     = {Neural network based adaboosting approach for hyperspectral data classification},
  booktitle = {2011 International Conference on Computer Science and Network Technology (ICCSNT)},
  year      = {2011},
  volume    = {1},
  pages     = {241--245},
  month     = {December},
  publisher = {IEEE},
}

@Unpublished{Shalev-Shwartz-2014,
  author = {S. Shalev-Shwartz},
  title  = {SelfieBoost: A Boosting Algorithm for Deep Learning},
  note   = {arXiv:1411.3436v1},
  month  = {Nov},
  year   = {2014},
}

@InProceedings{Schwenk-Bengio-1997,
  author    = {H. Schwenk and Y. Bengio},
  title     = {AdaBoosting neural networks: Application to on-line character recognition. InInternational},
  booktitle = {International Conference on Artificial Neural Networks},
  year      = {1997},
  pages     = {967--972},
  address   = {Berlin Heidelberg},
  month     = {Oct},
  publisher = {Springer},
}

@InProceedings{Simoff-1996,
  author    = {S.J. Simoff},
  title     = {Handling uncertainty in neural networks: an interval approach},
  booktitle = {IEEE International Conference on Neural Networks},
  year      = {1996},
  volume    = {1},
  pages     = {606--610},
  address   = {Washington, DC},
  publisher = {IEEE},
  owner     = {lev u},
  timestamp = {2017.02.24},
}

@InProceedings{Yao-Wang-Dong-2004,
  author    = {X. Yao and S. Wang and S. Dong},
  title     = {Approximation of interval models by neural networks},
  booktitle = {IEEE International Joint Conference on Neural Networks},
  year      = {2004},
  volume    = {2},
  pages     = {1027--1032},
  publisher = {IEEE},
  owner     = {lev u},
  timestamp = {2017.02.24},
}

@InProceedings{Fujioka-etal-1991,
  author    = {R. Fujioka and H. Ishibuchi and H. Tanaka and M. Omae},
  title     = {Learning Algorithm of Neural Networks for Interval-Valued Data},
  booktitle = {Proceedings of the 4th IFSA Congress. Part of Artificial Intelligence},
  year      = {1991},
  number    = {37--40},
  address   = {Brussels, Belgium},
  publisher = {IFSA},
  owner     = {lev u},
  timestamp = {2017.02.24},
}

@Article{Zarnani-etal-2014,
  author  = {Zarnani, A. and Musilek, P. and Heckenbergerova, J.},
  title   = {Clustering numerical weather forecasts to obtain statistical prediction intervals},
  journal = {Meteorological Applications},
  year    = {2014},
  volume  = {21},
  number  = {3},
  pages   = {605--618},
}

@PhdThesis{Xu-2010,
  author    = {W. Xu},
  title     = {Symbolic Data Analysis: Interval-valued Data Regression},
  school    = {University of Georgia},
  year      = {2010},
  type      = {phdthesis},
  address   = {Athens, Georgia},
  month     = aug,
  owner     = {lev u},
  timestamp = {2017.02.25},
}

@Article{Hajjar-Hamdan-2012,
  author    = {C. Hajjar and H. Hamdan},
  title     = {Kohonen Neural Networks for Interval-valued Data Clustering},
  journal   = {International Journal of Advanced Computer Science},
  year      = {2012},
  volume    = {2},
  number    = {11},
  pages     = {412--419},
  month     = {Nov.},
  abstract  = {The Self-Organizing Maps have been widely used as multidimensional unsupervised classifiers. The aim of this paper is to develop a self-organizing map for interval data. Due to the increasing use of such data in Data Mining, many clustering methods for interval data have been proposed this last decade. In this paper, we propose an algorithm to train the self-organizing map for interval data. We use the Euclidian distance to compare two vectors of intervals. In order to show the usefulness of our approach, we apply the self-organizing map on real interval data issued from meteorological stations in Lebanon.},
  booktitle = {2011 IEEE Workshop on Signal Processing Systems (SiPS)},
  owner     = {lev u},
  publisher = {IEEE},
  timestamp = {2017.02.25},
}

@Comment{jabref-meta: databaseType:bibtex;}
